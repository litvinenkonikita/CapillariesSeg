digraph {
	graph [size="249.75,249.75"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2688565184128 [label="
 (1, 2, 1024, 1024)" fillcolor=darkolivegreen1]
	2688565376576 -> 2688565196208 [dir=none]
	2688565196208 [label="input
 (1, 64, 1024, 1024)" fillcolor=orange]
	2688565376576 -> 2688565012624 [dir=none]
	2688565012624 [label="weight
 (2, 64, 1, 1)" fillcolor=orange]
	2688565376576 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (2,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565374560 -> 2688565376576
	2688565374560 -> 2688564813376 [dir=none]
	2688564813376 [label="result
 (1, 64, 1024, 1024)" fillcolor=orange]
	2688565374560 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565375184 -> 2688565374560
	2688565375184 -> 2688565195888 [dir=none]
	2688565195888 [label="input
 (1, 192, 1024, 1024)" fillcolor=orange]
	2688565375184 -> 2688565013104 [dir=none]
	2688565013104 [label="weight
 (64, 192, 3, 3)" fillcolor=orange]
	2688565375184 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565375376 -> 2688565375184
	2688565375376 [label="CatBackward0
------------
dim: 1"]
	2688565374752 -> 2688565375376
	2688565374752 [label="UpsampleBilinear2DBackward0
----------------------------------
align_corners :               True
output_size   :       (1024, 1024)
scales_h      :                2.0
scales_w      :                2.0
self_sym_sizes: (1, 128, 512, 512)"]
	2688565374080 -> 2688565374752
	2688565374080 -> 2688564815056 [dir=none]
	2688564815056 [label="result
 (1, 128, 512, 512)" fillcolor=orange]
	2688565374080 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565372688 -> 2688565374080
	2688565372688 -> 2688565189248 [dir=none]
	2688565189248 [label="input
 (1, 320, 512, 512)" fillcolor=orange]
	2688565372688 -> 2688565014944 [dir=none]
	2688565014944 [label="weight
 (128, 320, 3, 3)" fillcolor=orange]
	2688565372688 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565373456 -> 2688565372688
	2688565373456 [label="CatBackward0
------------
dim: 1"]
	2688565372832 -> 2688565373456
	2688565372832 [label="UpsampleBilinear2DBackward0
----------------------------------
align_corners :               True
output_size   :         (512, 512)
scales_h      :                2.0
scales_w      :                2.0
self_sym_sizes: (1, 256, 256, 256)"]
	2688565372544 -> 2688565372832
	2688565372544 -> 2688564815216 [dir=none]
	2688564815216 [label="result
 (1, 256, 256, 256)" fillcolor=orange]
	2688565372544 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565372256 -> 2688565372544
	2688565372256 -> 2688565195088 [dir=none]
	2688565195088 [label="input
 (1, 320, 256, 256)" fillcolor=orange]
	2688565372256 -> 2688565015344 [dir=none]
	2688565015344 [label="weight
 (256, 320, 3, 3)" fillcolor=orange]
	2688565372256 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565371920 -> 2688565372256
	2688565371920 [label="CatBackward0
------------
dim: 1"]
	2688565371296 -> 2688565371920
	2688565371296 [label="UpsampleBilinear2DBackward0
----------------------------------
align_corners :               True
output_size   :         (256, 256)
scales_h      :                2.0
scales_w      :                2.0
self_sym_sizes: (1, 256, 128, 128)"]
	2688565369568 -> 2688565371296
	2688565369568 -> 2688564815936 [dir=none]
	2688564815936 [label="result
 (1, 256, 128, 128)" fillcolor=orange]
	2688565369568 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565370336 -> 2688565369568
	2688565370336 -> 2688565195568 [dir=none]
	2688565195568 [label="input
 (1, 640, 128, 128)" fillcolor=orange]
	2688565370336 -> 2688565016304 [dir=none]
	2688565016304 [label="weight
 (256, 640, 3, 3)" fillcolor=orange]
	2688565370336 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565368944 -> 2688565370336
	2688565368944 [label="CatBackward0
------------
dim: 1"]
	2688565368320 -> 2688565368944
	2688565368320 [label="UpsampleBilinear2DBackward0
--------------------------------
align_corners :             True
output_size   :       (128, 128)
scales_h      :              2.0
scales_w      :              2.0
self_sym_sizes: (1, 512, 64, 64)"]
	2688565369136 -> 2688565368320
	2688565369136 -> 2688564815856 [dir=none]
	2688564815856 [label="result
 (1, 512, 64, 64)" fillcolor=orange]
	2688565369136 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565368800 -> 2688565369136
	2688565368800 -> 2688565194368 [dir=none]
	2688565194368 [label="input
 (1, 768, 64, 64)" fillcolor=orange]
	2688565368800 -> 2688565016784 [dir=none]
	2688565016784 [label="weight
 (512, 768, 3, 3)" fillcolor=orange]
	2688565368800 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565368512 -> 2688565368800
	2688565368512 [label="CatBackward0
------------
dim: 1"]
	2688565367888 -> 2688565368512
	2688565367888 [label="UpsampleBilinear2DBackward0
--------------------------------
align_corners :             True
output_size   :         (64, 64)
scales_h      :              2.0
scales_w      :              2.0
self_sym_sizes: (1, 512, 32, 32)"]
	2688565367216 -> 2688565367888
	2688565367216 -> 2688564815296 [dir=none]
	2688564815296 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	2688565367216 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565365824 -> 2688565367216
	2688565365824 -> 2688565194848 [dir=none]
	2688565194848 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565365824 -> 2688565017504 [dir=none]
	2688565017504 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	2688565365824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565366592 -> 2688565365824
	2688565366592 -> 2688564815696 [dir=none]
	2688564815696 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	2688565366592 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565365968 -> 2688565366592
	2688565365968 [label="AddBackward0
------------
alpha: 1"]
	2688565364576 -> 2688565365968
	2688565364576 -> 2688565193728 [dir=none]
	2688565193728 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565364576 -> 2688564817056 [dir=none]
	2688564817056 [label="result1
 (512)" fillcolor=orange]
	2688565364576 -> 2688564816336 [dir=none]
	2688564816336 [label="result2
 (512)" fillcolor=orange]
	2688565364576 -> 2688564813456 [dir=none]
	2688564813456 [label="result3
 (0)" fillcolor=orange]
	2688565364576 -> 2687733590560 [dir=none]
	2687733590560 [label="running_mean
 (512)" fillcolor=orange]
	2688565364576 -> 2688565141216 [dir=none]
	2688565141216 [label="running_var
 (512)" fillcolor=orange]
	2688565364576 -> 2688564816656 [dir=none]
	2688564816656 [label="weight
 (512)" fillcolor=orange]
	2688565364576 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565365392 -> 2688565364576
	2688565365392 -> 2688565193888 [dir=none]
	2688565193888 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565365392 -> 2688564816736 [dir=none]
	2688564816736 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2688565365392 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565364768 -> 2688565365392
	2688565364768 -> 2688556636640 [dir=none]
	2688556636640 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	2688565364768 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565364096 -> 2688565364768
	2688565364096 -> 2688565193968 [dir=none]
	2688565193968 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565364096 -> 2688552406768 [dir=none]
	2688552406768 [label="result1
 (512)" fillcolor=orange]
	2688565364096 -> 2688564816496 [dir=none]
	2688564816496 [label="result2
 (512)" fillcolor=orange]
	2688565364096 -> 2688564816896 [dir=none]
	2688564816896 [label="result3
 (0)" fillcolor=orange]
	2688565364096 -> 2688565002384 [dir=none]
	2688565002384 [label="running_mean
 (512)" fillcolor=orange]
	2688565364096 -> 2688565138656 [dir=none]
	2688565138656 [label="running_var
 (512)" fillcolor=orange]
	2688565364096 -> 2688564816096 [dir=none]
	2688564816096 [label="weight
 (512)" fillcolor=orange]
	2688565364096 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565363808 -> 2688565364096
	2688565363808 -> 2688565193328 [dir=none]
	2688565193328 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565363808 -> 2688564816176 [dir=none]
	2688564816176 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2688565363808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565366016 -> 2688565363808
	2688565366016 -> 2688564808896 [dir=none]
	2688564808896 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	2688565366016 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565362944 -> 2688565366016
	2688565362944 [label="AddBackward0
------------
alpha: 1"]
	2688565378304 -> 2688565362944
	2688565378304 -> 2688565193168 [dir=none]
	2688565193168 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565378304 -> 2688564817296 [dir=none]
	2688564817296 [label="result1
 (512)" fillcolor=orange]
	2688565378304 -> 2688564806416 [dir=none]
	2688564806416 [label="result2
 (512)" fillcolor=orange]
	2688565378304 -> 2688564808256 [dir=none]
	2688564808256 [label="result3
 (0)" fillcolor=orange]
	2688565378304 -> 2688565003184 [dir=none]
	2688565003184 [label="running_mean
 (512)" fillcolor=orange]
	2688565378304 -> 2688565007424 [dir=none]
	2688565007424 [label="running_var
 (512)" fillcolor=orange]
	2688565378304 -> 2688564815456 [dir=none]
	2688564815456 [label="weight
 (512)" fillcolor=orange]
	2688565378304 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565378112 -> 2688565378304
	2688565378112 -> 2688565192768 [dir=none]
	2688565192768 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565378112 -> 2688564815536 [dir=none]
	2688564815536 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2688565378112 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565377680 -> 2688565378112
	2688565377680 -> 2688564806976 [dir=none]
	2688564806976 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	2688565377680 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565377536 -> 2688565377680
	2688565377536 -> 2688565193408 [dir=none]
	2688565193408 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565377536 -> 2688565072000 [dir=none]
	2688565072000 [label="result1
 (512)" fillcolor=orange]
	2688565377536 -> 2688565071600 [dir=none]
	2688565071600 [label="result2
 (512)" fillcolor=orange]
	2688565377536 -> 2688565069840 [dir=none]
	2688565069840 [label="result3
 (0)" fillcolor=orange]
	2688565377536 -> 2687733588800 [dir=none]
	2687733588800 [label="running_mean
 (512)" fillcolor=orange]
	2688565377536 -> 2688565002704 [dir=none]
	2688565002704 [label="running_var
 (512)" fillcolor=orange]
	2688565377536 -> 2688564814816 [dir=none]
	2688564814816 [label="weight
 (512)" fillcolor=orange]
	2688565377536 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565377104 -> 2688565377536
	2688565377104 -> 2688565192448 [dir=none]
	2688565192448 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565377104 -> 2688564814896 [dir=none]
	2688564814896 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2688565377104 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565377488 -> 2688565377104
	2688565377488 -> 2688565073520 [dir=none]
	2688565073520 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	2688565377488 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565376960 -> 2688565377488
	2688565376960 [label="AddBackward0
------------
alpha: 1"]
	2688565376480 -> 2688565376960
	2688565376480 -> 2688565192848 [dir=none]
	2688565192848 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565376480 -> 2688565083840 [dir=none]
	2688565083840 [label="result1
 (512)" fillcolor=orange]
	2688565376480 -> 2688565077920 [dir=none]
	2688565077920 [label="result2
 (512)" fillcolor=orange]
	2688565376480 -> 2688565080960 [dir=none]
	2688565080960 [label="result3
 (0)" fillcolor=orange]
	2688565376480 -> 2688565004464 [dir=none]
	2688565004464 [label="running_mean
 (512)" fillcolor=orange]
	2688565376480 -> 2688565003584 [dir=none]
	2688565003584 [label="running_var
 (512)" fillcolor=orange]
	2688565376480 -> 2688564814336 [dir=none]
	2688564814336 [label="weight
 (512)" fillcolor=orange]
	2688565376480 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565376096 -> 2688565376480
	2688565376096 -> 2688565193088 [dir=none]
	2688565193088 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565376096 -> 2688564814416 [dir=none]
	2688564814416 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	2688565376096 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565375904 -> 2688565376096
	2688565375904 -> 2688565076720 [dir=none]
	2688565076720 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	2688565375904 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565375424 -> 2688565375904
	2688565375424 -> 2688565192048 [dir=none]
	2688565192048 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565375424 -> 2688565067840 [dir=none]
	2688565067840 [label="result1
 (512)" fillcolor=orange]
	2688565375424 -> 2688565079120 [dir=none]
	2688565079120 [label="result2
 (512)" fillcolor=orange]
	2688565375424 -> 2688565078720 [dir=none]
	2688565078720 [label="result3
 (0)" fillcolor=orange]
	2688565375424 -> 2688565005104 [dir=none]
	2688565005104 [label="running_mean
 (512)" fillcolor=orange]
	2688565375424 -> 2688565004384 [dir=none]
	2688565004384 [label="running_var
 (512)" fillcolor=orange]
	2688565375424 -> 2688564813696 [dir=none]
	2688564813696 [label="weight
 (512)" fillcolor=orange]
	2688565375424 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565375520 -> 2688565375424
	2688565375520 -> 2688565187648 [dir=none]
	2688565187648 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565375520 -> 2688564813776 [dir=none]
	2688564813776 [label="weight
 (512, 256, 3, 3)" fillcolor=orange]
	2688565375520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2688565375232 -> 2688565375520
	2688565375232 -> 2688565073120 [dir=none]
	2688565073120 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565375232 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565374848 -> 2688565375232
	2688565374848 [label="AddBackward0
------------
alpha: 1"]
	2688565374512 -> 2688565374848
	2688565374512 -> 2688565192128 [dir=none]
	2688565192128 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565374512 -> 2688565075040 [dir=none]
	2688565075040 [label="result1
 (256)" fillcolor=orange]
	2688565374512 -> 2688565077120 [dir=none]
	2688565077120 [label="result2
 (256)" fillcolor=orange]
	2688565374512 -> 2688565076800 [dir=none]
	2688565076800 [label="result3
 (0)" fillcolor=orange]
	2688565374512 -> 2688565005584 [dir=none]
	2688565005584 [label="running_mean
 (256)" fillcolor=orange]
	2688565374512 -> 2688565004784 [dir=none]
	2688565004784 [label="running_var
 (256)" fillcolor=orange]
	2688565374512 -> 2688564812336 [dir=none]
	2688564812336 [label="weight
 (256)" fillcolor=orange]
	2688565374512 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565374608 -> 2688565374512
	2688565374608 -> 2688565192368 [dir=none]
	2688565192368 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565374608 -> 2688564812416 [dir=none]
	2688564812416 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565374608 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565374272 -> 2688565374608
	2688565374272 -> 2688565080560 [dir=none]
	2688565080560 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565374272 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565374032 -> 2688565374272
	2688565374032 -> 2688565191488 [dir=none]
	2688565191488 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565374032 -> 2688565074000 [dir=none]
	2688565074000 [label="result1
 (256)" fillcolor=orange]
	2688565374032 -> 2688565075440 [dir=none]
	2688565075440 [label="result2
 (256)" fillcolor=orange]
	2688565374032 -> 2688565075120 [dir=none]
	2688565075120 [label="result3
 (0)" fillcolor=orange]
	2688565374032 -> 2688565006384 [dir=none]
	2688565006384 [label="running_mean
 (256)" fillcolor=orange]
	2688565374032 -> 2688565005504 [dir=none]
	2688565005504 [label="running_var
 (256)" fillcolor=orange]
	2688565374032 -> 2688564811696 [dir=none]
	2688564811696 [label="weight
 (256)" fillcolor=orange]
	2688565374032 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565373552 -> 2688565374032
	2688565373552 -> 2688565191728 [dir=none]
	2688565191728 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565373552 -> 2688564811776 [dir=none]
	2688564811776 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565373552 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565374896 -> 2688565373552
	2688565374896 -> 2688565077520 [dir=none]
	2688565077520 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565374896 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565373408 -> 2688565374896
	2688565373408 [label="AddBackward0
------------
alpha: 1"]
	2688565373120 -> 2688565373408
	2688565373120 -> 2688565191808 [dir=none]
	2688565191808 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565373120 -> 2688565073200 [dir=none]
	2688565073200 [label="result1
 (256)" fillcolor=orange]
	2688565373120 -> 2688565070960 [dir=none]
	2688565070960 [label="result2
 (256)" fillcolor=orange]
	2688565373120 -> 2688565073600 [dir=none]
	2688565073600 [label="result3
 (0)" fillcolor=orange]
	2688565373120 -> 2688565006784 [dir=none]
	2688565006784 [label="running_mean
 (256)" fillcolor=orange]
	2688565373120 -> 2688565006064 [dir=none]
	2688565006064 [label="running_var
 (256)" fillcolor=orange]
	2688565373120 -> 2688564811136 [dir=none]
	2688564811136 [label="weight
 (256)" fillcolor=orange]
	2688565373120 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565372640 -> 2688565373120
	2688565372640 -> 2688565191008 [dir=none]
	2688565191008 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565372640 -> 2688564811216 [dir=none]
	2688564811216 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565372640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565372496 -> 2688565372640
	2688565372496 -> 2688565072080 [dir=none]
	2688565072080 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565372496 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565372400 -> 2688565372496
	2688565372400 -> 2688565190608 [dir=none]
	2688565190608 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565372400 -> 2688565069920 [dir=none]
	2688565069920 [label="result1
 (256)" fillcolor=orange]
	2688565372400 -> 2688565070320 [dir=none]
	2688565070320 [label="result2
 (256)" fillcolor=orange]
	2688565372400 -> 2688565071680 [dir=none]
	2688565071680 [label="result3
 (0)" fillcolor=orange]
	2688565372400 -> 2688565007504 [dir=none]
	2688565007504 [label="running_mean
 (256)" fillcolor=orange]
	2688565372400 -> 2688565006704 [dir=none]
	2688565006704 [label="running_var
 (256)" fillcolor=orange]
	2688565372400 -> 2688564810496 [dir=none]
	2688564810496 [label="weight
 (256)" fillcolor=orange]
	2688565372400 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565372592 -> 2688565372400
	2688565372592 -> 2688565191088 [dir=none]
	2688565191088 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565372592 -> 2688564810576 [dir=none]
	2688564810576 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565372592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565373360 -> 2688565372592
	2688565373360 -> 2688565069520 [dir=none]
	2688565069520 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565373360 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565371776 -> 2688565373360
	2688565371776 [label="AddBackward0
------------
alpha: 1"]
	2688565371968 -> 2688565371776
	2688565371968 -> 2688565191408 [dir=none]
	2688565191408 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565371968 -> 2688565069120 [dir=none]
	2688565069120 [label="result1
 (256)" fillcolor=orange]
	2688565371968 -> 2688565067920 [dir=none]
	2688565067920 [label="result2
 (256)" fillcolor=orange]
	2688565371968 -> 2688565068320 [dir=none]
	2688565068320 [label="result3
 (0)" fillcolor=orange]
	2688565371968 -> 2688565008064 [dir=none]
	2688565008064 [label="running_mean
 (256)" fillcolor=orange]
	2688565371968 -> 2688565007184 [dir=none]
	2688565007184 [label="running_var
 (256)" fillcolor=orange]
	2688565371968 -> 2688564809856 [dir=none]
	2688564809856 [label="weight
 (256)" fillcolor=orange]
	2688565371968 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565371248 -> 2688565371968
	2688565371248 -> 2688565190128 [dir=none]
	2688565190128 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565371248 -> 2688564809936 [dir=none]
	2688564809936 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565371248 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565370768 -> 2688565371248
	2688565370768 -> 2688565068000 [dir=none]
	2688565068000 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565370768 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565370864 -> 2688565370768
	2688565370864 -> 2688565190208 [dir=none]
	2688565190208 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565370864 -> 2688565068560 [dir=none]
	2688565068560 [label="result1
 (256)" fillcolor=orange]
	2688565370864 -> 2688565068480 [dir=none]
	2688565068480 [label="result2
 (256)" fillcolor=orange]
	2688565370864 -> 2688565068080 [dir=none]
	2688565068080 [label="result3
 (0)" fillcolor=orange]
	2688565370864 -> 2688565008704 [dir=none]
	2688565008704 [label="running_mean
 (256)" fillcolor=orange]
	2688565370864 -> 2688565007984 [dir=none]
	2688565007984 [label="running_var
 (256)" fillcolor=orange]
	2688565370864 -> 2688564809216 [dir=none]
	2688564809216 [label="weight
 (256)" fillcolor=orange]
	2688565370864 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565370432 -> 2688565370864
	2688565370432 -> 2688565190528 [dir=none]
	2688565190528 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565370432 -> 2688564809296 [dir=none]
	2688564809296 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565370432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565371392 -> 2688565370432
	2688565371392 -> 2688565068880 [dir=none]
	2688565068880 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565371392 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565370000 -> 2688565371392
	2688565370000 [label="AddBackward0
------------
alpha: 1"]
	2688565366496 -> 2688565370000
	2688565366496 -> 2688565189488 [dir=none]
	2688565189488 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565366496 -> 2688565069280 [dir=none]
	2688565069280 [label="result1
 (256)" fillcolor=orange]
	2688565366496 -> 2688565069200 [dir=none]
	2688565069200 [label="result2
 (256)" fillcolor=orange]
	2688565366496 -> 2688565068960 [dir=none]
	2688565068960 [label="result3
 (0)" fillcolor=orange]
	2688565366496 -> 2688565009184 [dir=none]
	2688565009184 [label="running_mean
 (256)" fillcolor=orange]
	2688565366496 -> 2688565008464 [dir=none]
	2688565008464 [label="running_var
 (256)" fillcolor=orange]
	2688565366496 -> 2688564808576 [dir=none]
	2688564808576 [label="weight
 (256)" fillcolor=orange]
	2688565366496 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565369520 -> 2688565366496
	2688565369520 -> 2688565189408 [dir=none]
	2688565189408 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565369520 -> 2688564808656 [dir=none]
	2688564808656 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565369520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565369376 -> 2688565369520
	2688565369376 -> 2688565069600 [dir=none]
	2688565069600 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565369376 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565369280 -> 2688565369376
	2688565369280 -> 2688565189728 [dir=none]
	2688565189728 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565369280 -> 2688565070080 [dir=none]
	2688565070080 [label="result1
 (256)" fillcolor=orange]
	2688565369280 -> 2688565070000 [dir=none]
	2688565070000 [label="result2
 (256)" fillcolor=orange]
	2688565369280 -> 2688565069680 [dir=none]
	2688565069680 [label="result3
 (0)" fillcolor=orange]
	2688565369280 -> 2688565009984 [dir=none]
	2688565009984 [label="running_mean
 (256)" fillcolor=orange]
	2688565369280 -> 2688565009104 [dir=none]
	2688565009104 [label="running_var
 (256)" fillcolor=orange]
	2688565369280 -> 2688564807936 [dir=none]
	2688564807936 [label="weight
 (256)" fillcolor=orange]
	2688565369280 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565369472 -> 2688565369280
	2688565369472 -> 2688565189808 [dir=none]
	2688565189808 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565369472 -> 2688564808016 [dir=none]
	2688564808016 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565369472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565369808 -> 2688565369472
	2688565369808 -> 2688565070480 [dir=none]
	2688565070480 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565369808 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565368656 -> 2688565369808
	2688565368656 [label="AddBackward0
------------
alpha: 1"]
	2688565368848 -> 2688565368656
	2688565368848 -> 2688565188608 [dir=none]
	2688565188608 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565368848 -> 2688565070800 [dir=none]
	2688565070800 [label="result1
 (256)" fillcolor=orange]
	2688565368848 -> 2688565070240 [dir=none]
	2688565070240 [label="result2
 (256)" fillcolor=orange]
	2688565368848 -> 2688565070560 [dir=none]
	2688565070560 [label="result3
 (0)" fillcolor=orange]
	2688565368848 -> 2688565010064 [dir=none]
	2688565010064 [label="running_mean
 (256)" fillcolor=orange]
	2688565368848 -> 2688565009584 [dir=none]
	2688565009584 [label="running_var
 (256)" fillcolor=orange]
	2688565368848 -> 2688564807296 [dir=none]
	2688564807296 [label="weight
 (256)" fillcolor=orange]
	2688565368848 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565368128 -> 2688565368848
	2688565368128 -> 2688565189088 [dir=none]
	2688565189088 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565368128 -> 2688564807376 [dir=none]
	2688564807376 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565368128 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565367648 -> 2688565368128
	2688565367648 -> 2688565071040 [dir=none]
	2688565071040 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565367648 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565367744 -> 2688565367648
	2688565367744 -> 2688565188528 [dir=none]
	2688565188528 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565367744 -> 2688565071440 [dir=none]
	2688565071440 [label="result1
 (256)" fillcolor=orange]
	2688565367744 -> 2688565071360 [dir=none]
	2688565071360 [label="result2
 (256)" fillcolor=orange]
	2688565367744 -> 2688565071120 [dir=none]
	2688565071120 [label="result3
 (0)" fillcolor=orange]
	2688565367744 -> 2688564539808 [dir=none]
	2688564539808 [label="running_mean
 (256)" fillcolor=orange]
	2688565367744 -> 2688564543008 [dir=none]
	2688564543008 [label="running_var
 (256)" fillcolor=orange]
	2688565367744 -> 2688564806656 [dir=none]
	2688564806656 [label="weight
 (256)" fillcolor=orange]
	2688565367744 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565367312 -> 2688565367744
	2688565367312 -> 2688565187808 [dir=none]
	2688565187808 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565367312 -> 2688564806736 [dir=none]
	2688564806736 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565367312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565368272 -> 2688565367312
	2688565368272 -> 2688565071760 [dir=none]
	2688565071760 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565368272 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565367120 -> 2688565368272
	2688565367120 [label="AddBackward0
------------
alpha: 1"]
	2688565366688 -> 2688565367120
	2688565366688 -> 2688565187728 [dir=none]
	2688565187728 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565366688 -> 2688565072320 [dir=none]
	2688565072320 [label="result1
 (256)" fillcolor=orange]
	2688565366688 -> 2688565072240 [dir=none]
	2688565072240 [label="result2
 (256)" fillcolor=orange]
	2688565366688 -> 2688565071840 [dir=none]
	2688565071840 [label="result3
 (0)" fillcolor=orange]
	2688565366688 -> 2688564539168 [dir=none]
	2688564539168 [label="running_mean
 (256)" fillcolor=orange]
	2688565366688 -> 2688565006944 [dir=none]
	2688565006944 [label="running_var
 (256)" fillcolor=orange]
	2688565366688 -> 2688564806096 [dir=none]
	2688564806096 [label="weight
 (256)" fillcolor=orange]
	2688565366688 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565366400 -> 2688565366688
	2688565366400 -> 2688565188128 [dir=none]
	2688565188128 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565366400 -> 2688564806176 [dir=none]
	2688564806176 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	2688565366400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565366064 -> 2688565366400
	2688565366064 -> 2688565072640 [dir=none]
	2688565072640 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565366064 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565366160 -> 2688565366064
	2688565366160 -> 2688565188208 [dir=none]
	2688565188208 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565366160 -> 2688565073280 [dir=none]
	2688565073280 [label="result1
 (256)" fillcolor=orange]
	2688565366160 -> 2688565072960 [dir=none]
	2688565072960 [label="result2
 (256)" fillcolor=orange]
	2688565366160 -> 2688565072720 [dir=none]
	2688565072720 [label="result3
 (0)" fillcolor=orange]
	2688565366160 -> 2688564542288 [dir=none]
	2688564542288 [label="running_mean
 (256)" fillcolor=orange]
	2688565366160 -> 2688564543088 [dir=none]
	2688564543088 [label="running_var
 (256)" fillcolor=orange]
	2688565366160 -> 2688564543248 [dir=none]
	2688564543248 [label="weight
 (256)" fillcolor=orange]
	2688565366160 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565366352 -> 2688565366160
	2688565366352 -> 2688565189648 [dir=none]
	2688565189648 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565366352 -> 2688564543328 [dir=none]
	2688564543328 [label="weight
 (256, 128, 3, 3)" fillcolor=orange]
	2688565366352 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2688565365440 -> 2688565366352
	2688565365440 -> 2688565074240 [dir=none]
	2688565074240 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565365440 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565365152 -> 2688565365440
	2688565365152 [label="AddBackward0
------------
alpha: 1"]
	2688565365296 -> 2688565365152
	2688565365296 -> 2688565186768 [dir=none]
	2688565186768 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565365296 -> 2688565073760 [dir=none]
	2688565073760 [label="result1
 (128)" fillcolor=orange]
	2688565365296 -> 2688565073360 [dir=none]
	2688565073360 [label="result2
 (128)" fillcolor=orange]
	2688565365296 -> 2688565073680 [dir=none]
	2688565073680 [label="result3
 (0)" fillcolor=orange]
	2688565365296 -> 2688564541728 [dir=none]
	2688564541728 [label="running_mean
 (128)" fillcolor=orange]
	2688565365296 -> 2688564541808 [dir=none]
	2688564541808 [label="running_var
 (128)" fillcolor=orange]
	2688565365296 -> 2688564541888 [dir=none]
	2688564541888 [label="weight
 (128)" fillcolor=orange]
	2688565365296 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565364816 -> 2688565365296
	2688565364816 -> 2688565187168 [dir=none]
	2688565187168 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565364816 -> 2688564541968 [dir=none]
	2688564541968 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2688565364816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565365104 -> 2688565364816
	2688565365104 -> 2688565074560 [dir=none]
	2688565074560 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565365104 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565364384 -> 2688565365104
	2688565364384 -> 2688565187488 [dir=none]
	2688565187488 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565364384 -> 2688565075520 [dir=none]
	2688565075520 [label="result1
 (128)" fillcolor=orange]
	2688565364384 -> 2688565074160 [dir=none]
	2688565074160 [label="result2
 (128)" fillcolor=orange]
	2688565364384 -> 2688565074800 [dir=none]
	2688565074800 [label="result3
 (0)" fillcolor=orange]
	2688565364384 -> 2688564541088 [dir=none]
	2688564541088 [label="running_mean
 (128)" fillcolor=orange]
	2688565364384 -> 2688564541168 [dir=none]
	2688564541168 [label="running_var
 (128)" fillcolor=orange]
	2688565364384 -> 2688564541328 [dir=none]
	2688564541328 [label="weight
 (128)" fillcolor=orange]
	2688565364384 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565364240 -> 2688565364384
	2688565364240 -> 2688565186288 [dir=none]
	2688565186288 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565364240 -> 2688564541408 [dir=none]
	2688564541408 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2688565364240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565365728 -> 2688565364240
	2688565365728 -> 2688565075280 [dir=none]
	2688565075280 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565365728 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565363760 -> 2688565365728
	2688565363760 [label="AddBackward0
------------
alpha: 1"]
	2688565363616 -> 2688565363760
	2688565363616 -> 2688565185968 [dir=none]
	2688565185968 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565363616 -> 2688565075920 [dir=none]
	2688565075920 [label="result1
 (128)" fillcolor=orange]
	2688565363616 -> 2688565074400 [dir=none]
	2688565074400 [label="result2
 (128)" fillcolor=orange]
	2688565363616 -> 2688565075200 [dir=none]
	2688565075200 [label="result3
 (0)" fillcolor=orange]
	2688565363616 -> 2688564540448 [dir=none]
	2688564540448 [label="running_mean
 (128)" fillcolor=orange]
	2688565363616 -> 2688564540528 [dir=none]
	2688564540528 [label="running_var
 (128)" fillcolor=orange]
	2688565363616 -> 2688564540688 [dir=none]
	2688564540688 [label="weight
 (128)" fillcolor=orange]
	2688565363616 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565363856 -> 2688565363616
	2688565363856 -> 2688565186368 [dir=none]
	2688565186368 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565363856 -> 2688564540768 [dir=none]
	2688564540768 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2688565363856 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565362992 -> 2688565363856
	2688565362992 -> 2688565075600 [dir=none]
	2688565075600 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565362992 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565362848 -> 2688565362992
	2688565362848 -> 2688565186688 [dir=none]
	2688565186688 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565362848 -> 2688565072480 [dir=none]
	2688565072480 [label="result1
 (128)" fillcolor=orange]
	2688565362848 -> 2688565074880 [dir=none]
	2688565074880 [label="result2
 (128)" fillcolor=orange]
	2688565362848 -> 2688565076560 [dir=none]
	2688565076560 [label="result3
 (0)" fillcolor=orange]
	2688565362848 -> 2688564538608 [dir=none]
	2688564538608 [label="running_mean
 (128)" fillcolor=orange]
	2688565362848 -> 2688564539888 [dir=none]
	2688564539888 [label="running_var
 (128)" fillcolor=orange]
	2688565362848 -> 2688564540048 [dir=none]
	2688564540048 [label="weight
 (128)" fillcolor=orange]
	2688565362848 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565363040 -> 2688565362848
	2688565363040 -> 2688565185568 [dir=none]
	2688565185568 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565363040 -> 2688564540128 [dir=none]
	2688564540128 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2688565363040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565363568 -> 2688565363040
	2688565363568 -> 2688565076320 [dir=none]
	2688565076320 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565363568 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565313056 -> 2688565363568
	2688565313056 [label="AddBackward0
------------
alpha: 1"]
	2688565312720 -> 2688565313056
	2688565312720 -> 2688565185648 [dir=none]
	2688565185648 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565312720 -> 2688565076880 [dir=none]
	2688565076880 [label="result1
 (128)" fillcolor=orange]
	2688565312720 -> 2688565076000 [dir=none]
	2688565076000 [label="result2
 (128)" fillcolor=orange]
	2688565312720 -> 2688565077200 [dir=none]
	2688565077200 [label="result3
 (0)" fillcolor=orange]
	2688565312720 -> 2687733590640 [dir=none]
	2687733590640 [label="running_mean
 (128)" fillcolor=orange]
	2688565312720 -> 2688564539248 [dir=none]
	2688564539248 [label="running_var
 (128)" fillcolor=orange]
	2688565312720 -> 2688564539408 [dir=none]
	2688564539408 [label="weight
 (128)" fillcolor=orange]
	2688565312720 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565310992 -> 2688565312720
	2688565310992 -> 2688565184688 [dir=none]
	2688565184688 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565310992 -> 2688564539488 [dir=none]
	2688564539488 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2688565310992 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565310368 -> 2688565310992
	2688565310368 -> 2688565076960 [dir=none]
	2688565076960 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565310368 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565311184 -> 2688565310368
	2688565311184 -> 2688565185888 [dir=none]
	2688565185888 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565311184 -> 2688565077680 [dir=none]
	2688565077680 [label="result1
 (128)" fillcolor=orange]
	2688565311184 -> 2688565077280 [dir=none]
	2688565077280 [label="result2
 (128)" fillcolor=orange]
	2688565311184 -> 2688565078080 [dir=none]
	2688565078080 [label="result3
 (0)" fillcolor=orange]
	2688565311184 -> 2687733590480 [dir=none]
	2687733590480 [label="running_mean
 (128)" fillcolor=orange]
	2688565311184 -> 2687733581760 [dir=none]
	2687733581760 [label="running_var
 (128)" fillcolor=orange]
	2688565311184 -> 2688564538768 [dir=none]
	2688564538768 [label="weight
 (128)" fillcolor=orange]
	2688565311184 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565310848 -> 2688565311184
	2688565310848 -> 2688565185168 [dir=none]
	2688565185168 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565310848 -> 2688564538848 [dir=none]
	2688564538848 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2688565310848 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565311616 -> 2688565310848
	2688565311616 -> 2688565077760 [dir=none]
	2688565077760 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565311616 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565309936 -> 2688565311616
	2688565309936 [label="AddBackward0
------------
alpha: 1"]
	2688565309648 -> 2688565309936
	2688565309648 -> 2688565185248 [dir=none]
	2688565185248 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565309648 -> 2688565078800 [dir=none]
	2688565078800 [label="result1
 (128)" fillcolor=orange]
	2688565309648 -> 2688565078160 [dir=none]
	2688565078160 [label="result2
 (128)" fillcolor=orange]
	2688565309648 -> 2688565078400 [dir=none]
	2688565078400 [label="result3
 (0)" fillcolor=orange]
	2688565309648 -> 2687733580800 [dir=none]
	2687733580800 [label="running_mean
 (128)" fillcolor=orange]
	2688565309648 -> 2687733580720 [dir=none]
	2687733580720 [label="running_var
 (128)" fillcolor=orange]
	2688565309648 -> 2687733587040 [dir=none]
	2687733587040 [label="weight
 (128)" fillcolor=orange]
	2688565309648 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565308976 -> 2688565309648
	2688565308976 -> 2688565183968 [dir=none]
	2688565183968 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565308976 -> 2687733594000 [dir=none]
	2687733594000 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	2688565308976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565308352 -> 2688565308976
	2688565308352 -> 2688565079280 [dir=none]
	2688565079280 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565308352 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565308112 -> 2688565308352
	2688565308112 -> 2688565183008 [dir=none]
	2688565183008 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565308112 -> 2688565080080 [dir=none]
	2688565080080 [label="result1
 (128)" fillcolor=orange]
	2688565308112 -> 2688565078480 [dir=none]
	2688565078480 [label="result2
 (128)" fillcolor=orange]
	2688565308112 -> 2688565078880 [dir=none]
	2688565078880 [label="result3
 (0)" fillcolor=orange]
	2688565308112 -> 2687733588960 [dir=none]
	2687733588960 [label="running_mean
 (128)" fillcolor=orange]
	2688565308112 -> 2687733581680 [dir=none]
	2687733581680 [label="running_var
 (128)" fillcolor=orange]
	2688565308112 -> 2687733588560 [dir=none]
	2687733588560 [label="weight
 (128)" fillcolor=orange]
	2688565308112 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565307776 -> 2688565308112
	2688565307776 -> 2688565183888 [dir=none]
	2688565183888 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565307776 -> 2687733582720 [dir=none]
	2687733582720 [label="weight
 (128, 64, 3, 3)" fillcolor=orange]
	2688565307776 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2688565307152 -> 2688565307776
	2688565307152 -> 2688565079600 [dir=none]
	2688565079600 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	2688565307152 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565306480 -> 2688565307152
	2688565306480 [label="AddBackward0
------------
alpha: 1"]
	2688565306192 -> 2688565306480
	2688565306192 -> 2688565184288 [dir=none]
	2688565184288 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565306192 -> 2688565079200 [dir=none]
	2688565079200 [label="result1
 (64)" fillcolor=orange]
	2688565306192 -> 2688565081040 [dir=none]
	2688565081040 [label="result2
 (64)" fillcolor=orange]
	2688565306192 -> 2688565080000 [dir=none]
	2688565080000 [label="result3
 (0)" fillcolor=orange]
	2688565306192 -> 2687733589280 [dir=none]
	2687733589280 [label="running_mean
 (64)" fillcolor=orange]
	2688565306192 -> 2687733589200 [dir=none]
	2687733589200 [label="running_var
 (64)" fillcolor=orange]
	2688565306192 -> 2687733582160 [dir=none]
	2687733582160 [label="weight
 (64)" fillcolor=orange]
	2688565306192 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565305904 -> 2688565306192
	2688565305904 -> 2688565184368 [dir=none]
	2688565184368 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565305904 -> 2687733589120 [dir=none]
	2687733589120 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2688565305904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565305280 -> 2688565305904
	2688565305280 -> 2688565080320 [dir=none]
	2688565080320 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	2688565305280 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565304608 -> 2688565305280
	2688565304608 -> 2688565183248 [dir=none]
	2688565183248 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565304608 -> 2688565079680 [dir=none]
	2688565079680 [label="result1
 (64)" fillcolor=orange]
	2688565304608 -> 2688565081440 [dir=none]
	2688565081440 [label="result2
 (64)" fillcolor=orange]
	2688565304608 -> 2688565080720 [dir=none]
	2688565080720 [label="result3
 (0)" fillcolor=orange]
	2688565304608 -> 2687733589760 [dir=none]
	2687733589760 [label="running_mean
 (64)" fillcolor=orange]
	2688565304608 -> 2687733589680 [dir=none]
	2687733589680 [label="running_var
 (64)" fillcolor=orange]
	2688565304608 -> 2687733582480 [dir=none]
	2687733582480 [label="weight
 (64)" fillcolor=orange]
	2688565304608 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565304320 -> 2688565304608
	2688565304320 -> 2688565183568 [dir=none]
	2688565183568 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565304320 -> 2687733589440 [dir=none]
	2687733589440 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2688565304320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565306528 -> 2688565304320
	2688565306528 -> 2688565082320 [dir=none]
	2688565082320 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	2688565306528 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565302256 -> 2688565306528
	2688565302256 [label="AddBackward0
------------
alpha: 1"]
	2688565303024 -> 2688565302256
	2688565303024 -> 2688565183648 [dir=none]
	2688565183648 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565303024 -> 2688565080640 [dir=none]
	2688565080640 [label="result1
 (64)" fillcolor=orange]
	2688565303024 -> 2688565081760 [dir=none]
	2688565081760 [label="result2
 (64)" fillcolor=orange]
	2688565303024 -> 2688565081120 [dir=none]
	2688565081120 [label="result3
 (0)" fillcolor=orange]
	2688565303024 -> 2687733590080 [dir=none]
	2687733590080 [label="running_mean
 (64)" fillcolor=orange]
	2688565303024 -> 2687733590000 [dir=none]
	2687733590000 [label="running_var
 (64)" fillcolor=orange]
	2688565303024 -> 2687733582960 [dir=none]
	2687733582960 [label="weight
 (64)" fillcolor=orange]
	2688565303024 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565302400 -> 2688565303024
	2688565302400 -> 2688565182688 [dir=none]
	2688565182688 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565302400 -> 2687733589920 [dir=none]
	2687733589920 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2688565302400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565301776 -> 2688565302400
	2688565301776 -> 2688565083040 [dir=none]
	2688565083040 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	2688565301776 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565301488 -> 2688565301776
	2688565301488 -> 2688565185408 [dir=none]
	2688565185408 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565301488 -> 2688565082080 [dir=none]
	2688565082080 [label="result1
 (64)" fillcolor=orange]
	2688565301488 -> 2688565082400 [dir=none]
	2688565082400 [label="result2
 (64)" fillcolor=orange]
	2688565301488 -> 2688565082000 [dir=none]
	2688565082000 [label="result3
 (0)" fillcolor=orange]
	2688565301488 -> 2688565011504 [dir=none]
	2688565011504 [label="running_mean
 (64)" fillcolor=orange]
	2688565301488 -> 2688563967248 [dir=none]
	2688563967248 [label="running_var
 (64)" fillcolor=orange]
	2688565301488 -> 2687733583280 [dir=none]
	2687733583280 [label="weight
 (64)" fillcolor=orange]
	2688565301488 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565301200 -> 2688565301488
	2688565301200 -> 2688565182768 [dir=none]
	2688565182768 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565301200 -> 2687733590240 [dir=none]
	2687733590240 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2688565301200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565302976 -> 2688565301200
	2688565302976 -> 2688565083920 [dir=none]
	2688565083920 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	2688565302976 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565300240 -> 2688565302976
	2688565300240 [label="AddBackward0
------------
alpha: 1"]
	2688565299952 -> 2688565300240
	2688565299952 -> 2688565183168 [dir=none]
	2688565183168 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565299952 -> 2688565082720 [dir=none]
	2688565082720 [label="result1
 (64)" fillcolor=orange]
	2688565299952 -> 2688565083120 [dir=none]
	2688565083120 [label="result2
 (64)" fillcolor=orange]
	2688565299952 -> 2688565082640 [dir=none]
	2688565082640 [label="result3
 (0)" fillcolor=orange]
	2688565299952 -> 2688565011104 [dir=none]
	2688565011104 [label="running_mean
 (64)" fillcolor=orange]
	2688565299952 -> 2687733583520 [dir=none]
	2687733583520 [label="running_var
 (64)" fillcolor=orange]
	2688565299952 -> 2687733590400 [dir=none]
	2687733590400 [label="weight
 (64)" fillcolor=orange]
	2688565299952 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565299280 -> 2688565299952
	2688565299280 -> 2688565184208 [dir=none]
	2688565184208 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565299280 -> 2687733590320 [dir=none]
	2687733590320 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2688565299280 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565298704 -> 2688565299280
	2688565298704 -> 2688565076480 [dir=none]
	2688565076480 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	2688565298704 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565298032 -> 2688565298704
	2688565298032 -> 2688565184608 [dir=none]
	2688565184608 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565298032 -> 2688565083680 [dir=none]
	2688565083680 [label="result1
 (64)" fillcolor=orange]
	2688565298032 -> 2688565084000 [dir=none]
	2688565084000 [label="result2
 (64)" fillcolor=orange]
	2688565298032 -> 2688565083440 [dir=none]
	2688565083440 [label="result3
 (0)" fillcolor=orange]
	2688565298032 -> 2688565011904 [dir=none]
	2688565011904 [label="running_mean
 (64)" fillcolor=orange]
	2688565298032 -> 2687733590880 [dir=none]
	2687733590880 [label="running_var
 (64)" fillcolor=orange]
	2688565298032 -> 2687733583920 [dir=none]
	2687733583920 [label="weight
 (64)" fillcolor=orange]
	2688565298032 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565297744 -> 2688565298032
	2688565297744 -> 2688565182608 [dir=none]
	2688565182608 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565297744 -> 2687733590800 [dir=none]
	2687733590800 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2688565297744 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565299904 -> 2688565297744
	2688565299904 -> 2688565071280 [dir=none]
	2688565071280 [label="result1
 (1, 64, 256, 256)" fillcolor=orange]
	2688565299904 -> 2688565186608 [dir=none]
	2688565186608 [label="self
 (1, 64, 512, 512)" fillcolor=orange]
	2688565299904 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (3, 3)
padding    :         (1, 1)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	2688565299808 -> 2688565299904
	2688565299808 -> 2688565083600 [dir=none]
	2688565083600 [label="result
 (1, 64, 512, 512)" fillcolor=orange]
	2688565299808 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565303216 -> 2688565299808
	2688565303216 -> 2688565185008 [dir=none]
	2688565185008 [label="input
 (1, 64, 512, 512)" fillcolor=orange]
	2688565303216 -> 2688565076160 [dir=none]
	2688565076160 [label="result1
 (64)" fillcolor=orange]
	2688565303216 -> 2688565073920 [dir=none]
	2688565073920 [label="result2
 (64)" fillcolor=orange]
	2688565303216 -> 2688565079840 [dir=none]
	2688565079840 [label="result3
 (0)" fillcolor=orange]
	2688565303216 -> 2688565011984 [dir=none]
	2688565011984 [label="running_mean
 (64)" fillcolor=orange]
	2688565303216 -> 2687733584320 [dir=none]
	2687733584320 [label="running_var
 (64)" fillcolor=orange]
	2688565303216 -> 2687733591120 [dir=none]
	2687733591120 [label="weight
 (64)" fillcolor=orange]
	2688565303216 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565313440 -> 2688565303216
	2688565313440 -> 2688565186208 [dir=none]
	2688565186208 [label="input
 (1, 3, 1024, 1024)" fillcolor=orange]
	2688565313440 -> 2687733584240 [dir=none]
	2687733584240 [label="weight
 (64, 3, 7, 7)" fillcolor=orange]
	2688565313440 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2688565313152 -> 2688565313440
	2687733584240 [label="base_model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2687733584240 -> 2688565313152
	2688565313152 [label=AccumulateGrad]
	2688565312864 -> 2688565303216
	2687733591120 [label="base_model.bn1.weight
 (64)" fillcolor=lightblue]
	2687733591120 -> 2688565312864
	2688565312864 [label=AccumulateGrad]
	2688565297456 -> 2688565303216
	2687733591040 [label="base_model.bn1.bias
 (64)" fillcolor=lightblue]
	2687733591040 -> 2688565297456
	2688565297456 [label=AccumulateGrad]
	2688565303072 -> 2688565297744
	2687733590800 [label="base_model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2687733590800 -> 2688565303072
	2688565303072 [label=AccumulateGrad]
	2688565298080 -> 2688565298032
	2687733583920 [label="base_model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2687733583920 -> 2688565298080
	2688565298080 [label=AccumulateGrad]
	2688565297264 -> 2688565298032
	2687733590720 [label="base_model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2687733590720 -> 2688565297264
	2688565297264 [label=AccumulateGrad]
	2688565298656 -> 2688565299280
	2687733590320 [label="base_model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2687733590320 -> 2688565298656
	2688565298656 [label=AccumulateGrad]
	2688565299232 -> 2688565299952
	2687733590400 [label="base_model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2687733590400 -> 2688565299232
	2688565299232 [label=AccumulateGrad]
	2688565298512 -> 2688565299952
	2687733583680 [label="base_model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2687733583680 -> 2688565298512
	2688565298512 [label=AccumulateGrad]
	2688565299904 -> 2688565300240
	2688565300576 -> 2688565301200
	2687733590240 [label="base_model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2687733590240 -> 2688565300576
	2688565300576 [label=AccumulateGrad]
	2688565301152 -> 2688565301488
	2687733583280 [label="base_model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2687733583280 -> 2688565301152
	2688565301152 [label=AccumulateGrad]
	2688565301824 -> 2688565301488
	2687733590160 [label="base_model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2687733590160 -> 2688565301824
	2688565301824 [label=AccumulateGrad]
	2688565302112 -> 2688565302400
	2687733589920 [label="base_model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2687733589920 -> 2688565302112
	2688565302112 [label=AccumulateGrad]
	2688565302736 -> 2688565303024
	2687733582960 [label="base_model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2687733582960 -> 2688565302736
	2688565302736 [label=AccumulateGrad]
	2688565301632 -> 2688565303024
	2687733589840 [label="base_model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2687733589840 -> 2688565301632
	2688565301632 [label=AccumulateGrad]
	2688565302976 -> 2688565302256
	2688565303696 -> 2688565304320
	2687733589440 [label="base_model.layer1.2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2687733589440 -> 2688565303696
	2688565303696 [label=AccumulateGrad]
	2688565304656 -> 2688565304608
	2687733582480 [label="base_model.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2687733582480 -> 2688565304656
	2688565304656 [label=AccumulateGrad]
	2688565304944 -> 2688565304608
	2687733589360 [label="base_model.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2687733589360 -> 2688565304944
	2688565304944 [label=AccumulateGrad]
	2688565305232 -> 2688565305904
	2687733589120 [label="base_model.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2687733589120 -> 2688565305232
	2688565305232 [label=AccumulateGrad]
	2688565305856 -> 2688565306192
	2687733582160 [label="base_model.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2687733582160 -> 2688565305856
	2688565305856 [label=AccumulateGrad]
	2688565306240 -> 2688565306192
	2687733589040 [label="base_model.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2687733589040 -> 2688565306240
	2688565306240 [label=AccumulateGrad]
	2688565306528 -> 2688565306480
	2688565307104 -> 2688565307776
	2687733582720 [label="base_model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2687733582720 -> 2688565307104
	2688565307104 [label=AccumulateGrad]
	2688565307728 -> 2688565308112
	2687733588560 [label="base_model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2687733588560 -> 2688565307728
	2688565307728 [label=AccumulateGrad]
	2688565308400 -> 2688565308112
	2687733582640 [label="base_model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2687733582640 -> 2688565308400
	2688565308400 [label=AccumulateGrad]
	2688565308736 -> 2688565308976
	2687733594000 [label="base_model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2687733594000 -> 2688565308736
	2688565308736 [label=AccumulateGrad]
	2688565309360 -> 2688565309648
	2687733587040 [label="base_model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2687733587040 -> 2688565309360
	2688565309360 [label=AccumulateGrad]
	2688565309312 -> 2688565309648
	2687733593920 [label="base_model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2687733593920 -> 2688565309312
	2688565309312 [label=AccumulateGrad]
	2688565309600 -> 2688565309936
	2688565309600 -> 2688565184768 [dir=none]
	2688565184768 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565309600 -> 2688562368960 [dir=none]
	2688562368960 [label="result1
 (128)" fillcolor=orange]
	2688565309600 -> 2688562368800 [dir=none]
	2688562368800 [label="result2
 (128)" fillcolor=orange]
	2688565309600 -> 2688562368560 [dir=none]
	2688562368560 [label="result3
 (0)" fillcolor=orange]
	2688565309600 -> 2688565011424 [dir=none]
	2688565011424 [label="running_mean
 (128)" fillcolor=orange]
	2688565309600 -> 2687733582320 [dir=none]
	2687733582320 [label="running_var
 (128)" fillcolor=orange]
	2688565309600 -> 2687733588880 [dir=none]
	2687733588880 [label="weight
 (128)" fillcolor=orange]
	2688565309600 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565307488 -> 2688565309600
	2688565307488 -> 2688565183888 [dir=none]
	2688565183888 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565307488 -> 2687733582000 [dir=none]
	2687733582000 [label="weight
 (128, 64, 1, 1)" fillcolor=orange]
	2688565307488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2688565307152 -> 2688565307488
	2688565306816 -> 2688565307488
	2687733582000 [label="base_model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2687733582000 -> 2688565306816
	2688565306816 [label=AccumulateGrad]
	2688565308688 -> 2688565309600
	2687733588880 [label="base_model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2687733588880 -> 2688565308688
	2688565308688 [label=AccumulateGrad]
	2688565309024 -> 2688565309600
	2687733581920 [label="base_model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2687733581920 -> 2688565309024
	2688565309024 [label=AccumulateGrad]
	2688565310224 -> 2688565310848
	2688564538848 [label="base_model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2688564538848 -> 2688565310224
	2688565310224 [label=AccumulateGrad]
	2688565309744 -> 2688565311184
	2688564538768 [label="base_model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2688564538768 -> 2688565309744
	2688565309744 [label=AccumulateGrad]
	2688565311472 -> 2688565311184
	2688564538928 [label="base_model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2688564538928 -> 2688565311472
	2688565311472 [label=AccumulateGrad]
	2688565311808 -> 2688565310992
	2688564539488 [label="base_model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2688564539488 -> 2688565311808
	2688565311808 [label=AccumulateGrad]
	2688565312432 -> 2688565312720
	2688564539408 [label="base_model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2688564539408 -> 2688565312432
	2688565312432 [label=AccumulateGrad]
	2688565312384 -> 2688565312720
	2688564539568 [label="base_model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2688564539568 -> 2688565312384
	2688565312384 [label=AccumulateGrad]
	2688565311616 -> 2688565313056
	2688565313344 -> 2688565363040
	2688564540128 [label="base_model.layer2.2.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2688564540128 -> 2688565313344
	2688565313344 [label=AccumulateGrad]
	2688565302880 -> 2688565362848
	2688564540048 [label="base_model.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2688564540048 -> 2688565302880
	2688565302880 [label=AccumulateGrad]
	2688565302832 -> 2688565362848
	2688564540208 [label="base_model.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2688564540208 -> 2688565302832
	2688565302832 [label=AccumulateGrad]
	2688565363136 -> 2688565363856
	2688564540768 [label="base_model.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2688564540768 -> 2688565363136
	2688565363136 [label=AccumulateGrad]
	2688565363280 -> 2688565363616
	2688564540688 [label="base_model.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2688564540688 -> 2688565363280
	2688565363280 [label=AccumulateGrad]
	2688565363664 -> 2688565363616
	2688564540848 [label="base_model.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2688564540848 -> 2688565363664
	2688565363664 [label=AccumulateGrad]
	2688565363568 -> 2688565363760
	2688565364048 -> 2688565364240
	2688564541408 [label="base_model.layer2.3.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2688564541408 -> 2688565364048
	2688565364048 [label=AccumulateGrad]
	2688565364192 -> 2688565364384
	2688564541328 [label="base_model.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2688564541328 -> 2688565364192
	2688565364192 [label=AccumulateGrad]
	2688565364672 -> 2688565364384
	2688564541488 [label="base_model.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2688564541488 -> 2688565364672
	2688565364672 [label=AccumulateGrad]
	2688565364528 -> 2688565364816
	2688564541968 [label="base_model.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2688564541968 -> 2688565364528
	2688565364528 [label=AccumulateGrad]
	2688565365008 -> 2688565365296
	2688564541888 [label="base_model.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2688564541888 -> 2688565365008
	2688565365008 [label=AccumulateGrad]
	2688565365248 -> 2688565365296
	2688564542048 [label="base_model.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2688564542048 -> 2688565365248
	2688565365248 [label=AccumulateGrad]
	2688565365728 -> 2688565365152
	2688565365632 -> 2688565366352
	2688564543328 [label="base_model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2688564543328 -> 2688565365632
	2688565365632 [label=AccumulateGrad]
	2688565365776 -> 2688565366160
	2688564543248 [label="base_model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2688564543248 -> 2688565365776
	2688565365776 [label=AccumulateGrad]
	2688565362752 -> 2688565366160
	2688564543408 [label="base_model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2688564543408 -> 2688565362752
	2688565362752 [label=AccumulateGrad]
	2688565366256 -> 2688565366400
	2688564806176 [label="base_model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564806176 -> 2688565366256
	2688565366256 [label=AccumulateGrad]
	2688565366784 -> 2688565366688
	2688564806096 [label="base_model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2688564806096 -> 2688565366784
	2688565366784 [label=AccumulateGrad]
	2688565366736 -> 2688565366688
	2688564806256 [label="base_model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2688564806256 -> 2688565366736
	2688565366736 [label=AccumulateGrad]
	2688565366880 -> 2688565367120
	2688565366880 -> 2688565189008 [dir=none]
	2688565189008 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565366880 -> 2688565077440 [dir=none]
	2688565077440 [label="result1
 (256)" fillcolor=orange]
	2688565366880 -> 2688565072880 [dir=none]
	2688565072880 [label="result2
 (256)" fillcolor=orange]
	2688565366880 -> 2688565074720 [dir=none]
	2688565074720 [label="result3
 (0)" fillcolor=orange]
	2688565366880 -> 2688565010704 [dir=none]
	2688565010704 [label="running_mean
 (256)" fillcolor=orange]
	2688565366880 -> 2688564542448 [dir=none]
	2688564542448 [label="running_var
 (256)" fillcolor=orange]
	2688565366880 -> 2688564542608 [dir=none]
	2688564542608 [label="weight
 (256)" fillcolor=orange]
	2688565366880 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565365872 -> 2688565366880
	2688565365872 -> 2688565189648 [dir=none]
	2688565189648 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565365872 -> 2688564542528 [dir=none]
	2688564542528 [label="weight
 (256, 128, 1, 1)" fillcolor=orange]
	2688565365872 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2688565365440 -> 2688565365872
	2688565365488 -> 2688565365872
	2688564542528 [label="base_model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2688564542528 -> 2688565365488
	2688565365488 [label=AccumulateGrad]
	2688565366544 -> 2688565366880
	2688564542608 [label="base_model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2688564542608 -> 2688565366544
	2688565366544 [label=AccumulateGrad]
	2688565366976 -> 2688565366880
	2688564542688 [label="base_model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2688564542688 -> 2688565366976
	2688565366976 [label=AccumulateGrad]
	2688565367600 -> 2688565367312
	2688564806736 [label="base_model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564806736 -> 2688565367600
	2688565367600 [label=AccumulateGrad]
	2688565367504 -> 2688565367744
	2688564806656 [label="base_model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2688564806656 -> 2688565367504
	2688565367504 [label=AccumulateGrad]
	2688565368224 -> 2688565367744
	2688564806816 [label="base_model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2688564806816 -> 2688565368224
	2688565368224 [label=AccumulateGrad]
	2688565368032 -> 2688565368128
	2688564807376 [label="base_model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564807376 -> 2688565368032
	2688565368032 [label=AccumulateGrad]
	2688565368368 -> 2688565368848
	2688564807296 [label="base_model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2688564807296 -> 2688565368368
	2688565368368 [label=AccumulateGrad]
	2688565368416 -> 2688565368848
	2688564807456 [label="base_model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2688564807456 -> 2688565368416
	2688565368416 [label=AccumulateGrad]
	2688565368272 -> 2688565368656
	2688565368560 -> 2688565369472
	2688564808016 [label="base_model.layer3.2.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564808016 -> 2688565368560
	2688565368560 [label=AccumulateGrad]
	2688565368896 -> 2688565369280
	2688564807936 [label="base_model.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2688564807936 -> 2688565368896
	2688565368896 [label=AccumulateGrad]
	2688565369184 -> 2688565369280
	2688564808096 [label="base_model.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2688564808096 -> 2688565369184
	2688565369184 [label=AccumulateGrad]
	2688565369616 -> 2688565369520
	2688564808656 [label="base_model.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564808656 -> 2688565369616
	2688565369616 [label=AccumulateGrad]
	2688565369904 -> 2688565366496
	2688564808576 [label="base_model.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2688564808576 -> 2688565369904
	2688565369904 [label=AccumulateGrad]
	2688565369856 -> 2688565366496
	2688564808736 [label="base_model.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2688564808736 -> 2688565369856
	2688565369856 [label=AccumulateGrad]
	2688565369808 -> 2688565370000
	2688565370720 -> 2688565370432
	2688564809296 [label="base_model.layer3.3.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564809296 -> 2688565370720
	2688565370720 [label=AccumulateGrad]
	2688565370624 -> 2688565370864
	2688564809216 [label="base_model.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2688564809216 -> 2688565370624
	2688565370624 [label=AccumulateGrad]
	2688565371344 -> 2688565370864
	2688564809376 [label="base_model.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2688564809376 -> 2688565371344
	2688565371344 [label=AccumulateGrad]
	2688565371152 -> 2688565371248
	2688564809936 [label="base_model.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564809936 -> 2688565371152
	2688565371152 [label=AccumulateGrad]
	2688565371488 -> 2688565371968
	2688564809856 [label="base_model.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2688564809856 -> 2688565371488
	2688565371488 [label=AccumulateGrad]
	2688565371536 -> 2688565371968
	2688564810016 [label="base_model.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2688564810016 -> 2688565371536
	2688565371536 [label=AccumulateGrad]
	2688565371392 -> 2688565371776
	2688565371680 -> 2688565372592
	2688564810576 [label="base_model.layer3.4.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564810576 -> 2688565371680
	2688565371680 [label=AccumulateGrad]
	2688565372016 -> 2688565372400
	2688564810496 [label="base_model.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2688564810496 -> 2688565372016
	2688565372016 [label=AccumulateGrad]
	2688565372304 -> 2688565372400
	2688564810656 [label="base_model.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2688564810656 -> 2688565372304
	2688565372304 [label=AccumulateGrad]
	2688565372736 -> 2688565372640
	2688564811216 [label="base_model.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564811216 -> 2688565372736
	2688565372736 [label=AccumulateGrad]
	2688565373024 -> 2688565373120
	2688564811136 [label="base_model.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2688564811136 -> 2688565373024
	2688565373024 [label=AccumulateGrad]
	2688565372976 -> 2688565373120
	2688564811296 [label="base_model.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2688564811296 -> 2688565372976
	2688565372976 [label=AccumulateGrad]
	2688565373360 -> 2688565373408
	2688565373264 -> 2688565373552
	2688564811776 [label="base_model.layer3.5.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564811776 -> 2688565373264
	2688565373264 [label=AccumulateGrad]
	2688565373744 -> 2688565374032
	2688564811696 [label="base_model.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2688564811696 -> 2688565373744
	2688565373744 [label=AccumulateGrad]
	2688565373888 -> 2688565374032
	2688564811856 [label="base_model.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2688564811856 -> 2688565373888
	2688565373888 [label=AccumulateGrad]
	2688565374224 -> 2688565374608
	2688564812416 [label="base_model.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2688564812416 -> 2688565374224
	2688565374224 [label=AccumulateGrad]
	2688565374656 -> 2688565374512
	2688564812336 [label="base_model.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2688564812336 -> 2688565374656
	2688565374656 [label=AccumulateGrad]
	2688565375088 -> 2688565374512
	2688564812496 [label="base_model.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2688564812496 -> 2688565375088
	2688565375088 [label=AccumulateGrad]
	2688565374896 -> 2688565374848
	2688565375280 -> 2688565375520
	2688564813776 [label="base_model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2688564813776 -> 2688565375280
	2688565375280 [label=AccumulateGrad]
	2688565375472 -> 2688565375424
	2688564813696 [label="base_model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2688564813696 -> 2688565375472
	2688565375472 [label=AccumulateGrad]
	2688565375856 -> 2688565375424
	2688564813856 [label="base_model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2688564813856 -> 2688565375856
	2688565375856 [label=AccumulateGrad]
	2688565376336 -> 2688565376096
	2688564814416 [label="base_model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2688564814416 -> 2688565376336
	2688565376336 [label=AccumulateGrad]
	2688565376048 -> 2688565376480
	2688564814336 [label="base_model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2688564814336 -> 2688565376048
	2688565376048 [label=AccumulateGrad]
	2688565376240 -> 2688565376480
	2688564814496 [label="base_model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2688564814496 -> 2688565376240
	2688565376240 [label=AccumulateGrad]
	2688565376528 -> 2688565376960
	2688565376528 -> 2688565193648 [dir=none]
	2688565193648 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	2688565376528 -> 2688565068720 [dir=none]
	2688565068720 [label="result1
 (512)" fillcolor=orange]
	2688565376528 -> 2688565080240 [dir=none]
	2688565080240 [label="result2
 (512)" fillcolor=orange]
	2688565376528 -> 2688562368480 [dir=none]
	2688562368480 [label="result3
 (0)" fillcolor=orange]
	2688565376528 -> 2688565003984 [dir=none]
	2688565003984 [label="running_mean
 (512)" fillcolor=orange]
	2688565376528 -> 2688565003104 [dir=none]
	2688565003104 [label="running_var
 (512)" fillcolor=orange]
	2688565376528 -> 2688564813056 [dir=none]
	2688564813056 [label="weight
 (512)" fillcolor=orange]
	2688565376528 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	2688565375712 -> 2688565376528
	2688565375712 -> 2688565187648 [dir=none]
	2688565187648 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565375712 -> 2688564812976 [dir=none]
	2688564812976 [label="weight
 (512, 256, 1, 1)" fillcolor=orange]
	2688565375712 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	2688565375232 -> 2688565375712
	2688565374992 -> 2688565375712
	2688564812976 [label="base_model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2688564812976 -> 2688565374992
	2688565374992 [label=AccumulateGrad]
	2688565375760 -> 2688565376528
	2688564813056 [label="base_model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2688564813056 -> 2688565375760
	2688565375760 [label=AccumulateGrad]
	2688565376144 -> 2688565376528
	2688564813136 [label="base_model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2688564813136 -> 2688565376144
	2688565376144 [label=AccumulateGrad]
	2688565376768 -> 2688565377104
	2688564814896 [label="base_model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2688564814896 -> 2688565376768
	2688565376768 [label=AccumulateGrad]
	2688565377584 -> 2688565377536
	2688564814816 [label="base_model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2688564814816 -> 2688565377584
	2688565377584 [label=AccumulateGrad]
	2688565377776 -> 2688565377536
	2688564814976 [label="base_model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2688564814976 -> 2688565377776
	2688565377776 [label=AccumulateGrad]
	2688565377632 -> 2688565378112
	2688564815536 [label="base_model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2688564815536 -> 2688565377632
	2688565377632 [label=AccumulateGrad]
	2688565378448 -> 2688565378304
	2688564815456 [label="base_model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2688564815456 -> 2688565378448
	2688565378448 [label=AccumulateGrad]
	2688565378400 -> 2688565378304
	2688564815616 [label="base_model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2688564815616 -> 2688565378400
	2688565378400 [label=AccumulateGrad]
	2688565377488 -> 2688565362944
	2688565363232 -> 2688565363808
	2688564816176 [label="base_model.layer4.2.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2688564816176 -> 2688565363232
	2688565363232 [label=AccumulateGrad]
	2688565364144 -> 2688565364096
	2688564816096 [label="base_model.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2688564816096 -> 2688565364144
	2688565364144 [label=AccumulateGrad]
	2688565363328 -> 2688565364096
	2688564816256 [label="base_model.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2688564816256 -> 2688565363328
	2688565363328 [label=AccumulateGrad]
	2688565364720 -> 2688565365392
	2688564816736 [label="base_model.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2688564816736 -> 2688565364720
	2688565364720 [label=AccumulateGrad]
	2688565365344 -> 2688565364576
	2688564816656 [label="base_model.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2688564816656 -> 2688565365344
	2688565365344 [label=AccumulateGrad]
	2688565365680 -> 2688565364576
	2688564816816 [label="base_model.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2688564816816 -> 2688565365680
	2688565365680 [label=AccumulateGrad]
	2688565366016 -> 2688565365968
	2688565366928 -> 2688565365824
	2688565017504 [label="layer4_1x1.0.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	2688565017504 -> 2688565366928
	2688565366928 [label=AccumulateGrad]
	2688565366448 -> 2688565365824
	2688565017184 [label="layer4_1x1.0.bias
 (512)" fillcolor=lightblue]
	2688565017184 -> 2688565366448
	2688565366448 [label=AccumulateGrad]
	2688565367840 -> 2688565368512
	2688565367840 -> 2688562368640 [dir=none]
	2688562368640 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	2688565367840 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565366640 -> 2688565367840
	2688565366640 -> 2688565187648 [dir=none]
	2688565187648 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	2688565366640 -> 2688565017904 [dir=none]
	2688565017904 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	2688565366640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565375232 -> 2688565366640
	2688565365200 -> 2688565366640
	2688565017904 [label="layer3_1x1.0.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	2688565017904 -> 2688565365200
	2688565365200 [label=AccumulateGrad]
	2688565366304 -> 2688565366640
	2688565017584 [label="layer3_1x1.0.bias
 (256)" fillcolor=lightblue]
	2688565017584 -> 2688565366304
	2688565366304 [label=AccumulateGrad]
	2688565368464 -> 2688565368800
	2688565016784 [label="conv_up3.0.weight
 (512, 768, 3, 3)" fillcolor=lightblue]
	2688565016784 -> 2688565368464
	2688565368464 [label=AccumulateGrad]
	2688565369424 -> 2688565368800
	2688565016704 [label="conv_up3.0.bias
 (512)" fillcolor=lightblue]
	2688565016704 -> 2688565369424
	2688565369424 [label=AccumulateGrad]
	2688565369760 -> 2688565368944
	2688565369760 -> 2688562368880 [dir=none]
	2688562368880 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	2688565369760 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565367072 -> 2688565369760
	2688565367072 -> 2688565189648 [dir=none]
	2688565189648 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	2688565367072 -> 2688565018464 [dir=none]
	2688565018464 [label="weight
 (128, 128, 1, 1)" fillcolor=orange]
	2688565367072 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565365440 -> 2688565367072
	2688565367264 -> 2688565367072
	2688565018464 [label="layer2_1x1.0.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	2688565018464 -> 2688565367264
	2688565367264 [label=AccumulateGrad]
	2688565368176 -> 2688565367072
	2688565018384 [label="layer2_1x1.0.bias
 (128)" fillcolor=lightblue]
	2688565018384 -> 2688565368176
	2688565368176 [label=AccumulateGrad]
	2688565370384 -> 2688565370336
	2688565016304 [label="conv_up2.0.weight
 (256, 640, 3, 3)" fillcolor=lightblue]
	2688565016304 -> 2688565370384
	2688565370384 [label=AccumulateGrad]
	2688565370960 -> 2688565370336
	2688565016064 [label="conv_up2.0.bias
 (256)" fillcolor=lightblue]
	2688565016064 -> 2688565370960
	2688565370960 [label=AccumulateGrad]
	2688565370192 -> 2688565371920
	2688565370192 -> 2688565145376 [dir=none]
	2688565145376 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	2688565370192 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565370048 -> 2688565370192
	2688565370048 -> 2688565183888 [dir=none]
	2688565183888 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	2688565370048 -> 2688565017744 [dir=none]
	2688565017744 [label="weight
 (64, 64, 1, 1)" fillcolor=orange]
	2688565370048 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565307152 -> 2688565370048
	2688565367696 -> 2688565370048
	2688565017744 [label="layer1_1x1.0.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2688565017744 -> 2688565367696
	2688565367696 [label=AccumulateGrad]
	2688565369712 -> 2688565370048
	2688565010624 [label="layer1_1x1.0.bias
 (64)" fillcolor=lightblue]
	2688565010624 -> 2688565369712
	2688565369712 [label=AccumulateGrad]
	2688565370816 -> 2688565372256
	2688565015344 [label="conv_up1.0.weight
 (256, 320, 3, 3)" fillcolor=lightblue]
	2688565015344 -> 2688565370816
	2688565370816 [label=AccumulateGrad]
	2688565372880 -> 2688565372256
	2688565015264 [label="conv_up1.0.bias
 (256)" fillcolor=lightblue]
	2688565015264 -> 2688565372880
	2688565372880 [label=AccumulateGrad]
	2688565373168 -> 2688565373456
	2688565373168 -> 2688565139376 [dir=none]
	2688565139376 [label="result
 (1, 64, 512, 512)" fillcolor=orange]
	2688565373168 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565371584 -> 2688565373168
	2688565371584 -> 2688565186608 [dir=none]
	2688565186608 [label="input
 (1, 64, 512, 512)" fillcolor=orange]
	2688565371584 -> 2688565014544 [dir=none]
	2688565014544 [label="weight
 (64, 64, 1, 1)" fillcolor=orange]
	2688565371584 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565299808 -> 2688565371584
	2688565370672 -> 2688565371584
	2688565014544 [label="layer0_1x1.0.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2688565014544 -> 2688565370672
	2688565370672 [label=AccumulateGrad]
	2688565371632 -> 2688565371584
	2688565004944 [label="layer0_1x1.0.bias
 (64)" fillcolor=lightblue]
	2688565004944 -> 2688565371632
	2688565371632 [label=AccumulateGrad]
	2688565373792 -> 2688565372688
	2688565014944 [label="conv_up0.0.weight
 (128, 320, 3, 3)" fillcolor=lightblue]
	2688565014944 -> 2688565373792
	2688565373792 [label=AccumulateGrad]
	2688565373312 -> 2688565372688
	2688565014704 [label="conv_up0.0.bias
 (128)" fillcolor=lightblue]
	2688565014704 -> 2688565373312
	2688565373312 [label=AccumulateGrad]
	2688565374704 -> 2688565375376
	2688565374704 -> 2688565137936 [dir=none]
	2688565137936 [label="result
 (1, 64, 1024, 1024)" fillcolor=orange]
	2688565374704 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565373504 -> 2688565374704
	2688565373504 -> 2688557950400 [dir=none]
	2688557950400 [label="input
 (1, 64, 1024, 1024)" fillcolor=orange]
	2688565373504 -> 2688565013904 [dir=none]
	2688565013904 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	2688565373504 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565372208 -> 2688565373504
	2688565372208 -> 2688565137456 [dir=none]
	2688565137456 [label="result
 (1, 64, 1024, 1024)" fillcolor=orange]
	2688565372208 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	2688565367552 -> 2688565372208
	2688565367552 -> 2688565186208 [dir=none]
	2688565186208 [label="input
 (1, 3, 1024, 1024)" fillcolor=orange]
	2688565367552 -> 2688565014384 [dir=none]
	2688565014384 [label="weight
 (64, 3, 3, 3)" fillcolor=orange]
	2688565367552 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	2688565365056 -> 2688565367552
	2688565014384 [label="conv_original_size0.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2688565014384 -> 2688565365056
	2688565365056 [label=AccumulateGrad]
	2688565363952 -> 2688565367552
	2688565014304 [label="conv_original_size0.0.bias
 (64)" fillcolor=lightblue]
	2688565014304 -> 2688565363952
	2688565363952 [label=AccumulateGrad]
	2688565372064 -> 2688565373504
	2688565013904 [label="conv_original_size1.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2688565013904 -> 2688565372064
	2688565372064 [label=AccumulateGrad]
	2688565374416 -> 2688565373504
	2688565013504 [label="conv_original_size1.0.bias
 (64)" fillcolor=lightblue]
	2688565013504 -> 2688565374416
	2688565374416 [label=AccumulateGrad]
	2688565375328 -> 2688565375184
	2688565013104 [label="conv_original_size2.0.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	2688565013104 -> 2688565375328
	2688565375328 [label=AccumulateGrad]
	2688565376288 -> 2688565375184
	2688565013024 [label="conv_original_size2.0.bias
 (64)" fillcolor=lightblue]
	2688565013024 -> 2688565376288
	2688565376288 [label=AccumulateGrad]
	2688565375952 -> 2688565376576
	2688565012624 [label="conv_last.weight
 (2, 64, 1, 1)" fillcolor=lightblue]
	2688565012624 -> 2688565375952
	2688565375952 [label=AccumulateGrad]
	2688565376000 -> 2688565376576
	2688565012384 [label="conv_last.bias
 (2)" fillcolor=lightblue]
	2688565012384 -> 2688565376000
	2688565376000 [label=AccumulateGrad]
	2688565376576 -> 2688565184128
}
