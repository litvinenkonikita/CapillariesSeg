digraph {
	graph [size="154.95,154.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1861634250272 [label="
 (1, 1, 1024, 1024)" fillcolor=darkolivegreen1]
	1861634373808 -> 1861634250432 [dir=none]
	1861634250432 [label="input
 (1, 64, 1024, 1024)" fillcolor=orange]
	1861634373808 -> 1861634189296 [dir=none]
	1861634189296 [label="weight
 (1, 64, 1, 1)" fillcolor=orange]
	1861634373808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (1,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634372848 -> 1861634373808
	1861634372848 -> 1861633958560 [dir=none]
	1861633958560 [label="result
 (1, 64, 1024, 1024)" fillcolor=orange]
	1861634372848 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634373472 -> 1861634372848
	1861634373472 -> 1861634250512 [dir=none]
	1861634250512 [label="input
 (1, 192, 1024, 1024)" fillcolor=orange]
	1861634373472 -> 1861634189776 [dir=none]
	1861634189776 [label="weight
 (64, 192, 3, 3)" fillcolor=orange]
	1861634373472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634372608 -> 1861634373472
	1861634372608 [label="CatBackward0
------------
dim: 1"]
	1861634371984 -> 1861634372608
	1861634371984 [label="UpsampleBilinear2DBackward0
----------------------------------
align_corners :               True
output_size   :       (1024, 1024)
scales_h      :                2.0
scales_w      :                2.0
self_sym_sizes: (1, 128, 512, 512)"]
	1861634371312 -> 1861634371984
	1861634371312 -> 1861633961600 [dir=none]
	1861633961600 [label="result
 (1, 128, 512, 512)" fillcolor=orange]
	1861634371312 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634370976 -> 1861634371312
	1861634370976 -> 1861634248912 [dir=none]
	1861634248912 [label="input
 (1, 320, 512, 512)" fillcolor=orange]
	1861634370976 -> 1861634191696 [dir=none]
	1861634191696 [label="weight
 (128, 320, 3, 3)" fillcolor=orange]
	1861634370976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634370688 -> 1861634370976
	1861634370688 [label="CatBackward0
------------
dim: 1"]
	1861634370064 -> 1861634370688
	1861634370064 [label="UpsampleBilinear2DBackward0
----------------------------------
align_corners :               True
output_size   :         (512, 512)
scales_h      :                2.0
scales_w      :                2.0
self_sym_sizes: (1, 256, 256, 256)"]
	1861634369776 -> 1861634370064
	1861634369776 -> 1861633963040 [dir=none]
	1861633963040 [label="result
 (1, 256, 256, 256)" fillcolor=orange]
	1861634369776 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634369488 -> 1861634369776
	1861634369488 -> 1861634249392 [dir=none]
	1861634249392 [label="input
 (1, 320, 256, 256)" fillcolor=orange]
	1861634369488 -> 1861634192096 [dir=none]
	1861634192096 [label="weight
 (256, 320, 3, 3)" fillcolor=orange]
	1861634369488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634369152 -> 1861634369488
	1861634369152 [label="CatBackward0
------------
dim: 1"]
	1861634368528 -> 1861634369152
	1861634368528 [label="UpsampleBilinear2DBackward0
----------------------------------
align_corners :               True
output_size   :         (256, 256)
scales_h      :                2.0
scales_w      :                2.0
self_sym_sizes: (1, 256, 128, 128)"]
	1861634366752 -> 1861634368528
	1861634366752 -> 1861633963840 [dir=none]
	1861633963840 [label="result
 (1, 256, 128, 128)" fillcolor=orange]
	1861634366752 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634367520 -> 1861634366752
	1861634367520 -> 1861634248592 [dir=none]
	1861634248592 [label="input
 (1, 640, 128, 128)" fillcolor=orange]
	1861634367520 -> 1861634193056 [dir=none]
	1861634193056 [label="weight
 (256, 640, 3, 3)" fillcolor=orange]
	1861634367520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634367232 -> 1861634367520
	1861634367232 [label="CatBackward0
------------
dim: 1"]
	1861634366608 -> 1861634367232
	1861634366608 [label="UpsampleBilinear2DBackward0
--------------------------------
align_corners :             True
output_size   :       (128, 128)
scales_h      :              2.0
scales_w      :              2.0
self_sym_sizes: (1, 512, 64, 64)"]
	1861634364880 -> 1861634366608
	1861634364880 -> 1861633963120 [dir=none]
	1861633963120 [label="result
 (1, 512, 64, 64)" fillcolor=orange]
	1861634364880 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634365648 -> 1861634364880
	1861634365648 -> 1861634248192 [dir=none]
	1861634248192 [label="input
 (1, 768, 64, 64)" fillcolor=orange]
	1861634365648 -> 1861634193536 [dir=none]
	1861634193536 [label="weight
 (512, 768, 3, 3)" fillcolor=orange]
	1861634365648 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634364256 -> 1861634365648
	1861634364256 [label="CatBackward0
------------
dim: 1"]
	1861634363632 -> 1861634364256
	1861634363632 [label="UpsampleBilinear2DBackward0
--------------------------------
align_corners :             True
output_size   :         (64, 64)
scales_h      :              2.0
scales_w      :              2.0
self_sym_sizes: (1, 512, 32, 32)"]
	1861634364448 -> 1861634363632
	1861634364448 -> 1861633958160 [dir=none]
	1861633958160 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	1861634364448 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634363776 -> 1861634364448
	1861634363776 -> 1861634247472 [dir=none]
	1861634247472 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634363776 -> 1861634194336 [dir=none]
	1861634194336 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1861634363776 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634363488 -> 1861634363776
	1861634363488 -> 1861633959360 [dir=none]
	1861633959360 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	1861634363488 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634362864 -> 1861634363488
	1861634362864 [label="AddBackward0
------------
alpha: 1"]
	1861634362576 -> 1861634362864
	1861634362576 -> 1861634247072 [dir=none]
	1861634247072 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634362576 -> 1861633960080 [dir=none]
	1861633960080 [label="result1
 (512)" fillcolor=orange]
	1861634362576 -> 1861633956160 [dir=none]
	1861633956160 [label="result2
 (512)" fillcolor=orange]
	1861634362576 -> 1861633963520 [dir=none]
	1861633963520 [label="result3
 (0)" fillcolor=orange]
	1861634362576 -> 1861634185456 [dir=none]
	1861634185456 [label="running_mean
 (512)" fillcolor=orange]
	1861634362576 -> 1861634183936 [dir=none]
	1861634183936 [label="running_var
 (512)" fillcolor=orange]
	1861634362576 -> 1861633954960 [dir=none]
	1861633954960 [label="weight
 (512)" fillcolor=orange]
	1861634362576 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634362288 -> 1861634362576
	1861634362288 -> 1861634247872 [dir=none]
	1861634247872 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634362288 -> 1861633955040 [dir=none]
	1861633955040 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	1861634362288 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634361664 -> 1861634362288
	1861634361664 -> 1861633961200 [dir=none]
	1861633961200 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	1861634361664 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634360992 -> 1861634361664
	1861634360992 -> 1861634247392 [dir=none]
	1861634247392 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634360992 -> 1861633957440 [dir=none]
	1861633957440 [label="result1
 (512)" fillcolor=orange]
	1861634360992 -> 1861633956480 [dir=none]
	1861633956480 [label="result2
 (512)" fillcolor=orange]
	1861634360992 -> 1861633957760 [dir=none]
	1861633957760 [label="result3
 (0)" fillcolor=orange]
	1861634360992 -> 1861634185216 [dir=none]
	1861634185216 [label="running_mean
 (512)" fillcolor=orange]
	1861634360992 -> 1861634184736 [dir=none]
	1861634184736 [label="running_var
 (512)" fillcolor=orange]
	1861634360992 -> 1861633954320 [dir=none]
	1861633954320 [label="weight
 (512)" fillcolor=orange]
	1861634360992 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634360704 -> 1861634360992
	1861634360704 -> 1861634246592 [dir=none]
	1861634246592 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634360704 -> 1861633954400 [dir=none]
	1861633954400 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	1861634360704 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634362912 -> 1861634360704
	1861634362912 -> 1861633959680 [dir=none]
	1861633959680 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	1861634362912 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634359744 -> 1861634362912
	1861634359744 [label="AddBackward0
------------
alpha: 1"]
	1861634359456 -> 1861634359744
	1861634359456 -> 1861634246192 [dir=none]
	1861634246192 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634359456 -> 1861633956640 [dir=none]
	1861633956640 [label="result1
 (512)" fillcolor=orange]
	1861634359456 -> 1861633956960 [dir=none]
	1861633956960 [label="result2
 (512)" fillcolor=orange]
	1861634359456 -> 1861633956560 [dir=none]
	1861633956560 [label="result3
 (0)" fillcolor=orange]
	1861634359456 -> 1861634186656 [dir=none]
	1861634186656 [label="running_mean
 (512)" fillcolor=orange]
	1861634359456 -> 1861634185856 [dir=none]
	1861634185856 [label="running_var
 (512)" fillcolor=orange]
	1861634359456 -> 1861633953760 [dir=none]
	1861633953760 [label="weight
 (512)" fillcolor=orange]
	1861634359456 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634375632 -> 1861634359456
	1861634375632 -> 1861634246272 [dir=none]
	1861634246272 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634375632 -> 1861633953840 [dir=none]
	1861633953840 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	1861634375632 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634375104 -> 1861634375632
	1861634375104 -> 1861633957520 [dir=none]
	1861633957520 [label="result
 (1, 512, 32, 32)" fillcolor=orange]
	1861634375104 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634374816 -> 1861634375104
	1861634374816 -> 1861634246672 [dir=none]
	1861634246672 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634374816 -> 1861633958400 [dir=none]
	1861633958400 [label="result1
 (512)" fillcolor=orange]
	1861634374816 -> 1861633957840 [dir=none]
	1861633957840 [label="result2
 (512)" fillcolor=orange]
	1861634374816 -> 1861633957920 [dir=none]
	1861633957920 [label="result3
 (0)" fillcolor=orange]
	1861634374816 -> 1861634187136 [dir=none]
	1861634187136 [label="running_mean
 (512)" fillcolor=orange]
	1861634374816 -> 1861634186336 [dir=none]
	1861634186336 [label="running_var
 (512)" fillcolor=orange]
	1861634374816 -> 1861633953120 [dir=none]
	1861633953120 [label="weight
 (512)" fillcolor=orange]
	1861634374816 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634374960 -> 1861634374816
	1861634374960 -> 1861634246992 [dir=none]
	1861634246992 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634374960 -> 1861633953200 [dir=none]
	1861633953200 [label="weight
 (512, 256, 3, 3)" fillcolor=orange]
	1861634374960 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	1861634374528 -> 1861634374960
	1861634374528 -> 1861633958720 [dir=none]
	1861633958720 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	1861634374528 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634374768 -> 1861634374528
	1861634374768 [label="AddBackward0
------------
alpha: 1"]
	1861634374048 -> 1861634374768
	1861634374048 -> 1861634245552 [dir=none]
	1861634245552 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634374048 -> 1861633959120 [dir=none]
	1861633959120 [label="result1
 (256)" fillcolor=orange]
	1861634374048 -> 1861633958800 [dir=none]
	1861633958800 [label="result2
 (256)" fillcolor=orange]
	1861634374048 -> 1861633959040 [dir=none]
	1861633959040 [label="result3
 (0)" fillcolor=orange]
	1861634374048 -> 1861634187376 [dir=none]
	1861634187376 [label="running_mean
 (256)" fillcolor=orange]
	1861634374048 -> 1861634187056 [dir=none]
	1861634187056 [label="running_var
 (256)" fillcolor=orange]
	1861634374048 -> 1861633951760 [dir=none]
	1861633951760 [label="weight
 (256)" fillcolor=orange]
	1861634374048 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634373904 -> 1861634374048
	1861634373904 -> 1861634245632 [dir=none]
	1861634245632 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634373904 -> 1861633951840 [dir=none]
	1861633951840 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	1861634373904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634373760 -> 1861634373904
	1861634373760 -> 1861633958320 [dir=none]
	1861633958320 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	1861634373760 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634373280 -> 1861634373760
	1861634373280 -> 1861634245072 [dir=none]
	1861634245072 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634373280 -> 1861633960320 [dir=none]
	1861633960320 [label="result1
 (256)" fillcolor=orange]
	1861634373280 -> 1861633959760 [dir=none]
	1861633959760 [label="result2
 (256)" fillcolor=orange]
	1861634373280 -> 1861633960240 [dir=none]
	1861633960240 [label="result3
 (0)" fillcolor=orange]
	1861634373280 -> 1860786290608 [dir=none]
	1860786290608 [label="running_mean
 (256)" fillcolor=orange]
	1861634373280 -> 1861633720240 [dir=none]
	1861633720240 [label="running_var
 (256)" fillcolor=orange]
	1861634373280 -> 1861633951120 [dir=none]
	1861633951120 [label="weight
 (256)" fillcolor=orange]
	1861634373280 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634372944 -> 1861634373280
	1861634372944 -> 1861634245952 [dir=none]
	1861634245952 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634372944 -> 1861633951200 [dir=none]
	1861633951200 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	1861634372944 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634374336 -> 1861634372944
	1861634374336 -> 1861633959440 [dir=none]
	1861633959440 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	1861634374336 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634372656 -> 1861634374336
	1861634372656 [label="AddBackward0
------------
alpha: 1"]
	1861634372320 -> 1861634372656
	1861634372320 -> 1861634249952 [dir=none]
	1861634249952 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634372320 -> 1861633959840 [dir=none]
	1861633959840 [label="result1
 (256)" fillcolor=orange]
	1861634372320 -> 1861633960960 [dir=none]
	1861633960960 [label="result2
 (256)" fillcolor=orange]
	1861634372320 -> 1861633961280 [dir=none]
	1861633961280 [label="result3
 (0)" fillcolor=orange]
	1861634372320 -> 1860786278768 [dir=none]
	1860786278768 [label="running_mean
 (256)" fillcolor=orange]
	1861634372320 -> 1861634187856 [dir=none]
	1861634187856 [label="running_var
 (256)" fillcolor=orange]
	1861634372320 -> 1861633950560 [dir=none]
	1861633950560 [label="weight
 (256)" fillcolor=orange]
	1861634372320 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634372512 -> 1861634372320
	1861634372512 -> 1861634245392 [dir=none]
	1861634245392 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634372512 -> 1861633950640 [dir=none]
	1861633950640 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	1861634372512 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634371696 -> 1861634372512
	1861634371696 -> 1861633959520 [dir=none]
	1861633959520 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	1861634371696 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634371888 -> 1861634371696
	1861634371888 -> 1861634244992 [dir=none]
	1861634244992 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634371888 -> 1861633960640 [dir=none]
	1861633960640 [label="result1
 (256)" fillcolor=orange]
	1861634371888 -> 1861633962160 [dir=none]
	1861633962160 [label="result2
 (256)" fillcolor=orange]
	1861634371888 -> 1861633960880 [dir=none]
	1861633960880 [label="result3
 (0)" fillcolor=orange]
	1861634371888 -> 1860786278288 [dir=none]
	1860786278288 [label="running_mean
 (256)" fillcolor=orange]
	1861634371888 -> 1861634189536 [dir=none]
	1861634189536 [label="running_var
 (256)" fillcolor=orange]
	1861634371888 -> 1861633949920 [dir=none]
	1861633949920 [label="weight
 (256)" fillcolor=orange]
	1861634371888 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634371552 -> 1861634371888
	1861634371552 -> 1861634244672 [dir=none]
	1861634244672 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634371552 -> 1861633950000 [dir=none]
	1861633950000 [label="weight
 (256, 128, 3, 3)" fillcolor=orange]
	1861634371552 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	1861634371120 -> 1861634371552
	1861634371120 -> 1861633962080 [dir=none]
	1861633962080 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	1861634371120 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634371168 -> 1861634371120
	1861634371168 [label="AddBackward0
------------
alpha: 1"]
	1861634370784 -> 1861634371168
	1861634370784 -> 1861634246512 [dir=none]
	1861634246512 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634370784 -> 1861633961760 [dir=none]
	1861633961760 [label="result1
 (128)" fillcolor=orange]
	1861634370784 -> 1861633960720 [dir=none]
	1861633960720 [label="result2
 (128)" fillcolor=orange]
	1861634370784 -> 1861633961360 [dir=none]
	1861633961360 [label="result3
 (0)" fillcolor=orange]
	1861634370784 -> 1860786288768 [dir=none]
	1860786288768 [label="running_mean
 (128)" fillcolor=orange]
	1861634370784 -> 1861633718960 [dir=none]
	1861633718960 [label="running_var
 (128)" fillcolor=orange]
	1861634370784 -> 1861633719120 [dir=none]
	1861633719120 [label="weight
 (128)" fillcolor=orange]
	1861634370784 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634370592 -> 1861634370784
	1861634370592 -> 1861634246912 [dir=none]
	1861634246912 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634370592 -> 1861633719200 [dir=none]
	1861633719200 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	1861634370592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634370160 -> 1861634370592
	1861634370160 -> 1861633963200 [dir=none]
	1861633963200 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	1861634370160 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634369872 -> 1861634370160
	1861634369872 -> 1861634248432 [dir=none]
	1861634248432 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634369872 -> 1861633962480 [dir=none]
	1861633962480 [label="result1
 (128)" fillcolor=orange]
	1861634369872 -> 1861633961680 [dir=none]
	1861633961680 [label="result2
 (128)" fillcolor=orange]
	1861634369872 -> 1861633962560 [dir=none]
	1861633962560 [label="result3
 (0)" fillcolor=orange]
	1861634369872 -> 1860786277888 [dir=none]
	1860786277888 [label="running_mean
 (128)" fillcolor=orange]
	1861634369872 -> 1860786277248 [dir=none]
	1860786277248 [label="running_var
 (128)" fillcolor=orange]
	1861634369872 -> 1860786284368 [dir=none]
	1860786284368 [label="weight
 (128)" fillcolor=orange]
	1861634369872 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634370016 -> 1861634369872
	1861634370016 -> 1861634248832 [dir=none]
	1861634248832 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634370016 -> 1860786284288 [dir=none]
	1860786284288 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	1861634370016 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634370928 -> 1861634370016
	1861634370928 -> 1861633962800 [dir=none]
	1861633962800 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	1861634370928 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634369248 -> 1861634370928
	1861634369248 [label="AddBackward0
------------
alpha: 1"]
	1861634369392 -> 1861634369248
	1861634369392 -> 1861634250752 [dir=none]
	1861634250752 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634369392 -> 1861633953440 [dir=none]
	1861633953440 [label="result1
 (128)" fillcolor=orange]
	1861634369392 -> 1861633962880 [dir=none]
	1861633962880 [label="result2
 (128)" fillcolor=orange]
	1861634369392 -> 1861633963600 [dir=none]
	1861633963600 [label="result3
 (0)" fillcolor=orange]
	1861634369392 -> 1860786283568 [dir=none]
	1860786283568 [label="running_mean
 (128)" fillcolor=orange]
	1861634369392 -> 1860786283488 [dir=none]
	1860786283488 [label="running_var
 (128)" fillcolor=orange]
	1861634369392 -> 1860786290288 [dir=none]
	1860786290288 [label="weight
 (128)" fillcolor=orange]
	1861634369392 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634368912 -> 1861634369392
	1861634368912 -> 1861634250352 [dir=none]
	1861634250352 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634368912 -> 1860786276928 [dir=none]
	1860786276928 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	1861634368912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634368720 -> 1861634368912
	1861634368720 -> 1861633963280 [dir=none]
	1861633963280 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	1861634368720 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634368288 -> 1861634368720
	1861634368288 -> 1861634254032 [dir=none]
	1861634254032 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634368288 -> 1861633964720 [dir=none]
	1861633964720 [label="result1
 (128)" fillcolor=orange]
	1861634368288 -> 1861633964000 [dir=none]
	1861633964000 [label="result2
 (128)" fillcolor=orange]
	1861634368288 -> 1861634248032 [dir=none]
	1861634248032 [label="result3
 (0)" fillcolor=orange]
	1861634368288 -> 1860786277568 [dir=none]
	1860786277568 [label="running_mean
 (128)" fillcolor=orange]
	1861634368288 -> 1860786277168 [dir=none]
	1860786277168 [label="running_var
 (128)" fillcolor=orange]
	1861634368288 -> 1860786283648 [dir=none]
	1860786283648 [label="weight
 (128)" fillcolor=orange]
	1861634368288 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634367952 -> 1861634368288
	1861634367952 -> 1861634249232 [dir=none]
	1861634249232 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634367952 -> 1860786290528 [dir=none]
	1860786290528 [label="weight
 (128, 64, 3, 3)" fillcolor=orange]
	1861634367952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	1861634367856 -> 1861634367952
	1861634367856 -> 1861633957200 [dir=none]
	1861633957200 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	1861634367856 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634367184 -> 1861634367856
	1861634367184 [label="AddBackward0
------------
alpha: 1"]
	1861634367040 -> 1861634367184
	1861634367040 -> 1861634252832 [dir=none]
	1861634252832 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634367040 -> 1861633963440 [dir=none]
	1861633963440 [label="result1
 (64)" fillcolor=orange]
	1861634367040 -> 1861633964480 [dir=none]
	1861633964480 [label="result2
 (64)" fillcolor=orange]
	1861634367040 -> 1861633964800 [dir=none]
	1861633964800 [label="result3
 (0)" fillcolor=orange]
	1861634367040 -> 1860786278048 [dir=none]
	1860786278048 [label="running_mean
 (64)" fillcolor=orange]
	1861634367040 -> 1860786277968 [dir=none]
	1860786277968 [label="running_var
 (64)" fillcolor=orange]
	1861634367040 -> 1860786284448 [dir=none]
	1860786284448 [label="weight
 (64)" fillcolor=orange]
	1861634367040 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634367280 -> 1861634367040
	1861634367280 -> 1861634251152 [dir=none]
	1861634251152 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634367280 -> 1860786277728 [dir=none]
	1860786277728 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	1861634367280 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634366368 -> 1861634367280
	1861634366368 -> 1861633955840 [dir=none]
	1861633955840 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	1861634366368 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634366080 -> 1861634366368
	1861634366080 -> 1861634252432 [dir=none]
	1861634252432 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634366080 -> 1861633964080 [dir=none]
	1861633964080 [label="result1
 (64)" fillcolor=orange]
	1861634366080 -> 1861633965120 [dir=none]
	1861633965120 [label="result2
 (64)" fillcolor=orange]
	1861634366080 -> 1861633965920 [dir=none]
	1861633965920 [label="result3
 (0)" fillcolor=orange]
	1861634366080 -> 1861634184496 [dir=none]
	1861634184496 [label="running_mean
 (64)" fillcolor=orange]
	1861634366080 -> 1861633124336 [dir=none]
	1861633124336 [label="running_var
 (64)" fillcolor=orange]
	1861634366080 -> 1860786284768 [dir=none]
	1860786284768 [label="weight
 (64)" fillcolor=orange]
	1861634366080 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634366224 -> 1861634366080
	1861634366224 -> 1861634254752 [dir=none]
	1861634254752 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634366224 -> 1860786278208 [dir=none]
	1860786278208 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	1861634366224 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634366992 -> 1861634366224
	1861634366992 -> 1861633962720 [dir=none]
	1861633962720 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	1861634366992 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634365456 -> 1861634366992
	1861634365456 [label="AddBackward0
------------
alpha: 1"]
	1861634365600 -> 1861634365456
	1861634365600 -> 1861634256592 [dir=none]
	1861634256592 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634365600 -> 1861633965200 [dir=none]
	1861633965200 [label="result1
 (64)" fillcolor=orange]
	1861634365600 -> 1861633966000 [dir=none]
	1861633966000 [label="result2
 (64)" fillcolor=orange]
	1861634365600 -> 1861633965520 [dir=none]
	1861633965520 [label="result3
 (0)" fillcolor=orange]
	1861634365600 -> 1861634187696 [dir=none]
	1861634187696 [label="running_mean
 (64)" fillcolor=orange]
	1861634365600 -> 1860786285328 [dir=none]
	1860786285328 [label="running_var
 (64)" fillcolor=orange]
	1861634365600 -> 1860786278528 [dir=none]
	1860786278528 [label="weight
 (64)" fillcolor=orange]
	1861634365600 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634365120 -> 1861634365600
	1861634365120 -> 1861634256272 [dir=none]
	1861634256272 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634365120 -> 1860786278448 [dir=none]
	1860786278448 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	1861634365120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634365408 -> 1861634365120
	1861634365408 -> 1861633950960 [dir=none]
	1861633950960 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	1861634365408 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634364688 -> 1861634365408
	1861634364688 -> 1861634254352 [dir=none]
	1861634254352 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634364688 -> 1861633960480 [dir=none]
	1861633960480 [label="result1
 (64)" fillcolor=orange]
	1861634364688 -> 1861633950320 [dir=none]
	1861633950320 [label="result2
 (64)" fillcolor=orange]
	1861634364688 -> 1861633949760 [dir=none]
	1861633949760 [label="result3
 (0)" fillcolor=orange]
	1861634364688 -> 1861634188416 [dir=none]
	1861634188416 [label="running_mean
 (64)" fillcolor=orange]
	1861634364688 -> 1860786285488 [dir=none]
	1860786285488 [label="running_var
 (64)" fillcolor=orange]
	1861634364688 -> 1860786278688 [dir=none]
	1860786278688 [label="weight
 (64)" fillcolor=orange]
	1861634364688 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634364544 -> 1861634364688
	1861634364544 -> 1861634257232 [dir=none]
	1861634257232 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634364544 -> 1860786278608 [dir=none]
	1860786278608 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	1861634364544 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634366032 -> 1861634364544
	1861634366032 -> 1861633950400 [dir=none]
	1861633950400 [label="result1
 (1, 64, 256, 256)" fillcolor=orange]
	1861634366032 -> 1861634257152 [dir=none]
	1861634257152 [label="self
 (1, 64, 512, 512)" fillcolor=orange]
	1861634366032 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (3, 3)
padding    :         (1, 1)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	1861634363872 -> 1861634366032
	1861634363872 -> 1861633952160 [dir=none]
	1861633952160 [label="result
 (1, 64, 512, 512)" fillcolor=orange]
	1861634363872 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634363920 -> 1861634363872
	1861634363920 -> 1861634257072 [dir=none]
	1861634257072 [label="input
 (1, 64, 512, 512)" fillcolor=orange]
	1861634363920 -> 1861633950800 [dir=none]
	1861633950800 [label="result1
 (64)" fillcolor=orange]
	1861634363920 -> 1861633965600 [dir=none]
	1861633965600 [label="result2
 (64)" fillcolor=orange]
	1861634363920 -> 1861633952000 [dir=none]
	1861633952000 [label="result3
 (0)" fillcolor=orange]
	1861634363920 -> 1861634188656 [dir=none]
	1861634188656 [label="running_mean
 (64)" fillcolor=orange]
	1861634363920 -> 1860786285728 [dir=none]
	1860786285728 [label="running_var
 (64)" fillcolor=orange]
	1861634363920 -> 1860786279088 [dir=none]
	1860786279088 [label="weight
 (64)" fillcolor=orange]
	1861634363920 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634363584 -> 1861634363920
	1861634363584 -> 1861634257472 [dir=none]
	1861634257472 [label="input
 (1, 3, 1024, 1024)" fillcolor=orange]
	1861634363584 -> 1860786279168 [dir=none]
	1860786279168 [label="weight
 (64, 3, 7, 7)" fillcolor=orange]
	1861634363584 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (3, 3)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	1861634363440 -> 1861634363584
	1860786279168 [label="base_model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1860786279168 -> 1861634363440
	1861634363440 [label=AccumulateGrad]
	1861634363968 -> 1861634363920
	1860786279088 [label="base_model.bn1.weight
 (64)" fillcolor=lightblue]
	1860786279088 -> 1861634363968
	1861634363968 [label=AccumulateGrad]
	1861634364208 -> 1861634363920
	1860786285648 [label="base_model.bn1.bias
 (64)" fillcolor=lightblue]
	1860786285648 -> 1861634364208
	1861634364208 [label=AccumulateGrad]
	1861634364352 -> 1861634364544
	1860786278608 [label="base_model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1860786278608 -> 1861634364352
	1861634364352 [label=AccumulateGrad]
	1861634364496 -> 1861634364688
	1860786278688 [label="base_model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1860786278688 -> 1861634364496
	1861634364496 [label=AccumulateGrad]
	1861634364976 -> 1861634364688
	1860786285168 [label="base_model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1860786285168 -> 1861634364976
	1861634364976 [label=AccumulateGrad]
	1861634364832 -> 1861634365120
	1860786278448 [label="base_model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1860786278448 -> 1861634364832
	1861634364832 [label=AccumulateGrad]
	1861634365312 -> 1861634365600
	1860786278528 [label="base_model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1860786278528 -> 1861634365312
	1861634365312 [label=AccumulateGrad]
	1861634365552 -> 1861634365600
	1860786285008 [label="base_model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1860786285008 -> 1861634365552
	1861634365552 [label=AccumulateGrad]
	1861634366032 -> 1861634365456
	1861634365792 -> 1861634366224
	1860786278208 [label="base_model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1860786278208 -> 1861634365792
	1861634365792 [label=AccumulateGrad]
	1861634366656 -> 1861634366080
	1860786284768 [label="base_model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1860786284768 -> 1861634366656
	1861634366656 [label=AccumulateGrad]
	1861634366416 -> 1861634366080
	1860786278128 [label="base_model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1860786278128 -> 1861634366416
	1861634366416 [label=AccumulateGrad]
	1861634366560 -> 1861634367280
	1860786277728 [label="base_model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1860786277728 -> 1861634366560
	1861634366560 [label=AccumulateGrad]
	1861634366704 -> 1861634367040
	1860786284448 [label="base_model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1860786284448 -> 1861634366704
	1861634366704 [label=AccumulateGrad]
	1861634367088 -> 1861634367040
	1860786277648 [label="base_model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1860786277648 -> 1861634367088
	1861634367088 [label=AccumulateGrad]
	1861634366992 -> 1861634367184
	1861634368144 -> 1861634367952
	1860786290528 [label="base_model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1860786290528 -> 1861634368144
	1861634368144 [label=AccumulateGrad]
	1861634368336 -> 1861634368288
	1860786283648 [label="base_model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1860786283648 -> 1861634368336
	1861634368336 [label=AccumulateGrad]
	1861634368768 -> 1861634368288
	1860786290448 [label="base_model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1860786290448 -> 1861634368768
	1861634368768 [label=AccumulateGrad]
	1861634368624 -> 1861634368912
	1860786276928 [label="base_model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1860786276928 -> 1861634368624
	1861634368624 [label=AccumulateGrad]
	1861634369056 -> 1861634369392
	1860786290288 [label="base_model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1860786290288 -> 1861634369056
	1861634369056 [label=AccumulateGrad]
	1861634369296 -> 1861634369392
	1860786276848 [label="base_model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1860786276848 -> 1861634369296
	1861634369296 [label=AccumulateGrad]
	1861634369344 -> 1861634369248
	1861634369344 -> 1861634247232 [dir=none]
	1861634247232 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634369344 -> 1861633951360 [dir=none]
	1861633951360 [label="result1
 (128)" fillcolor=orange]
	1861634369344 -> 1861633951520 [dir=none]
	1861633951520 [label="result2
 (128)" fillcolor=orange]
	1861634369344 -> 1861633953600 [dir=none]
	1861633953600 [label="result3
 (0)" fillcolor=orange]
	1861634369344 -> 1861634191936 [dir=none]
	1861634191936 [label="running_mean
 (128)" fillcolor=orange]
	1861634369344 -> 1860786284048 [dir=none]
	1860786284048 [label="running_var
 (128)" fillcolor=orange]
	1861634369344 -> 1860786277408 [dir=none]
	1860786277408 [label="weight
 (128)" fillcolor=orange]
	1861634369344 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634368096 -> 1861634369344
	1861634368096 -> 1861634249232 [dir=none]
	1861634249232 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634368096 -> 1860786283968 [dir=none]
	1860786283968 [label="weight
 (128, 64, 1, 1)" fillcolor=orange]
	1861634368096 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	1861634367856 -> 1861634368096
	1861634367904 -> 1861634368096
	1860786283968 [label="base_model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1860786283968 -> 1861634367904
	1861634367904 [label=AccumulateGrad]
	1861634368576 -> 1861634369344
	1860786277408 [label="base_model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1860786277408 -> 1861634368576
	1861634368576 [label=AccumulateGrad]
	1861634368960 -> 1861634369344
	1860786277328 [label="base_model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1860786277328 -> 1861634368960
	1861634368960 [label=AccumulateGrad]
	1861634369584 -> 1861634370016
	1860786284288 [label="base_model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1860786284288 -> 1861634369584
	1861634369584 [label=AccumulateGrad]
	1861634369968 -> 1861634369872
	1860786284368 [label="base_model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1860786284368 -> 1861634369968
	1861634369968 [label=AccumulateGrad]
	1861634370208 -> 1861634369872
	1860786282448 [label="base_model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1860786282448 -> 1861634370208
	1861634370208 [label=AccumulateGrad]
	1861634370304 -> 1861634370592
	1861633719200 [label="base_model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1861633719200 -> 1861634370304
	1861634370304 [label=AccumulateGrad]
	1861634370496 -> 1861634370784
	1861633719120 [label="base_model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1861633719120 -> 1861634370496
	1861634370496 [label=AccumulateGrad]
	1861634370448 -> 1861634370784
	1861633719280 [label="base_model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1861633719280 -> 1861634370448
	1861634370448 [label=AccumulateGrad]
	1861634370928 -> 1861634371168
	1861634371072 -> 1861634371552
	1861633950000 [label="base_model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1861633950000 -> 1861634371072
	1861634371072 [label=AccumulateGrad]
	1861634371792 -> 1861634371888
	1861633949920 [label="base_model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1861633949920 -> 1861634371792
	1861634371792 [label=AccumulateGrad]
	1861634371744 -> 1861634371888
	1861633950080 [label="base_model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1861633950080 -> 1861634371744
	1861634371744 [label=AccumulateGrad]
	1861634372080 -> 1861634372512
	1861633950640 [label="base_model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1861633950640 -> 1861634372080
	1861634372080 [label=AccumulateGrad]
	1861634372464 -> 1861634372320
	1861633950560 [label="base_model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1861633950560 -> 1861634372464
	1861634372464 [label=AccumulateGrad]
	1861634372368 -> 1861634372320
	1861633950720 [label="base_model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1861633950720 -> 1861634372368
	1861634372368 [label=AccumulateGrad]
	1861634372704 -> 1861634372656
	1861634372704 -> 1861634245152 [dir=none]
	1861634245152 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634372704 -> 1861633953360 [dir=none]
	1861633953360 [label="result1
 (256)" fillcolor=orange]
	1861634372704 -> 1861633954000 [dir=none]
	1861633954000 [label="result2
 (256)" fillcolor=orange]
	1861634372704 -> 1861633952960 [dir=none]
	1861633952960 [label="result3
 (0)" fillcolor=orange]
	1861634372704 -> 1861634188176 [dir=none]
	1861634188176 [label="running_mean
 (256)" fillcolor=orange]
	1861634372704 -> 1861633719680 [dir=none]
	1861633719680 [label="running_var
 (256)" fillcolor=orange]
	1861634372704 -> 1861633719840 [dir=none]
	1861633719840 [label="weight
 (256)" fillcolor=orange]
	1861634372704 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634371456 -> 1861634372704
	1861634371456 -> 1861634244672 [dir=none]
	1861634244672 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634371456 -> 1861633719760 [dir=none]
	1861633719760 [label="weight
 (256, 128, 1, 1)" fillcolor=orange]
	1861634371456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	1861634371120 -> 1861634371456
	1861634371216 -> 1861634371456
	1861633719760 [label="base_model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1861633719760 -> 1861634371216
	1861634371216 [label=AccumulateGrad]
	1861634372032 -> 1861634372704
	1861633719840 [label="base_model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1861633719840 -> 1861634372032
	1861634372032 [label=AccumulateGrad]
	1861634372176 -> 1861634372704
	1861633719920 [label="base_model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1861633719920 -> 1861634372176
	1861634372176 [label=AccumulateGrad]
	1861634373040 -> 1861634372944
	1861633951200 [label="base_model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1861633951200 -> 1861634373040
	1861634373040 [label=AccumulateGrad]
	1861634373328 -> 1861634373280
	1861633951120 [label="base_model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1861633951120 -> 1861634373328
	1861634373328 [label=AccumulateGrad]
	1861634373664 -> 1861634373280
	1861633951280 [label="base_model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1861633951280 -> 1861634373664
	1861634373664 [label=AccumulateGrad]
	1861634373712 -> 1861634373904
	1861633951840 [label="base_model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1861633951840 -> 1861634373712
	1861634373712 [label=AccumulateGrad]
	1861634368672 -> 1861634374048
	1861633951760 [label="base_model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1861633951760 -> 1861634368672
	1861634368672 [label=AccumulateGrad]
	1861634372416 -> 1861634374048
	1861633951920 [label="base_model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1861633951920 -> 1861634372416
	1861634372416 [label=AccumulateGrad]
	1861634374336 -> 1861634374768
	1861634374480 -> 1861634374960
	1861633953200 [label="base_model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1861633953200 -> 1861634374480
	1861634374480 [label=AccumulateGrad]
	1861634375392 -> 1861634374816
	1861633953120 [label="base_model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1861633953120 -> 1861634375392
	1861634375392 [label=AccumulateGrad]
	1861634375152 -> 1861634374816
	1861633953280 [label="base_model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1861633953280 -> 1861634375152
	1861634375152 [label=AccumulateGrad]
	1861634375296 -> 1861634375632
	1861633953840 [label="base_model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1861633953840 -> 1861634375296
	1861634375296 [label=AccumulateGrad]
	1861634367808 -> 1861634359456
	1861633953760 [label="base_model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1861633953760 -> 1861634367808
	1861634367808 [label=AccumulateGrad]
	1861634359504 -> 1861634359456
	1861633953920 [label="base_model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1861633953920 -> 1861634359504
	1861634359504 [label=AccumulateGrad]
	1861634359792 -> 1861634359744
	1861634359792 -> 1861634247792 [dir=none]
	1861634247792 [label="input
 (1, 512, 32, 32)" fillcolor=orange]
	1861634359792 -> 1861633953520 [dir=none]
	1861633953520 [label="result1
 (512)" fillcolor=orange]
	1861634359792 -> 1861633950160 [dir=none]
	1861633950160 [label="result2
 (512)" fillcolor=orange]
	1861634359792 -> 1861633951600 [dir=none]
	1861633951600 [label="result3
 (0)" fillcolor=orange]
	1861634359792 -> 1861634185936 [dir=none]
	1861634185936 [label="running_mean
 (512)" fillcolor=orange]
	1861634359792 -> 1861634185136 [dir=none]
	1861634185136 [label="running_var
 (512)" fillcolor=orange]
	1861634359792 -> 1861633952480 [dir=none]
	1861633952480 [label="weight
 (512)" fillcolor=orange]
	1861634359792 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	1861634374672 -> 1861634359792
	1861634374672 -> 1861634246992 [dir=none]
	1861634246992 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634374672 -> 1861633952400 [dir=none]
	1861633952400 [label="weight
 (512, 256, 1, 1)" fillcolor=orange]
	1861634374672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (2, 2)
transposed        :          False
weight            : [saved tensor]"]
	1861634374528 -> 1861634374672
	1861634374576 -> 1861634374672
	1861633952400 [label="base_model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1861633952400 -> 1861634374576
	1861634374576 [label=AccumulateGrad]
	1861634375536 -> 1861634359792
	1861633952480 [label="base_model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1861633952480 -> 1861634375536
	1861634375536 [label=AccumulateGrad]
	1861634375440 -> 1861634359792
	1861633952560 [label="base_model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1861633952560 -> 1861634375440
	1861634375440 [label=AccumulateGrad]
	1861634360080 -> 1861634360704
	1861633954400 [label="base_model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1861633954400 -> 1861634360080
	1861634360080 [label=AccumulateGrad]
	1861634361040 -> 1861634360992
	1861633954320 [label="base_model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1861633954320 -> 1861634361040
	1861634361040 [label=AccumulateGrad]
	1861634361328 -> 1861634360992
	1861633954480 [label="base_model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1861633954480 -> 1861634361328
	1861634361328 [label=AccumulateGrad]
	1861634361616 -> 1861634362288
	1861633955040 [label="base_model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1861633955040 -> 1861634361616
	1861634361616 [label=AccumulateGrad]
	1861634362240 -> 1861634362576
	1861633954960 [label="base_model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1861633954960 -> 1861634362240
	1861634362240 [label=AccumulateGrad]
	1861634362624 -> 1861634362576
	1861633955120 [label="base_model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1861633955120 -> 1861634362624
	1861634362624 [label=AccumulateGrad]
	1861634362912 -> 1861634362864
	1861634363824 -> 1861634363776
	1861634194336 [label="layer4_1x1.0.weight
 (512, 512, 1, 1)" fillcolor=lightblue]
	1861634194336 -> 1861634363824
	1861634363824 [label=AccumulateGrad]
	1861634364736 -> 1861634363776
	1861634193936 [label="layer4_1x1.0.bias
 (512)" fillcolor=lightblue]
	1861634193936 -> 1861634364736
	1861634364736 [label=AccumulateGrad]
	1861634365072 -> 1861634364256
	1861634365072 -> 1861633952640 [dir=none]
	1861633952640 [label="result
 (1, 256, 64, 64)" fillcolor=orange]
	1861634365072 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634363536 -> 1861634365072
	1861634363536 -> 1861634246992 [dir=none]
	1861634246992 [label="input
 (1, 256, 64, 64)" fillcolor=orange]
	1861634363536 -> 1861634194816 [dir=none]
	1861634194816 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1861634363536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634374528 -> 1861634363536
	1861634363200 -> 1861634363536
	1861634194816 [label="layer3_1x1.0.weight
 (256, 256, 1, 1)" fillcolor=lightblue]
	1861634194816 -> 1861634363200
	1861634363200 [label=AccumulateGrad]
	1861634363248 -> 1861634363536
	1861634194736 [label="layer3_1x1.0.bias
 (256)" fillcolor=lightblue]
	1861634194736 -> 1861634363248
	1861634363248 [label=AccumulateGrad]
	1861634365696 -> 1861634365648
	1861634193536 [label="conv_up3.0.weight
 (512, 768, 3, 3)" fillcolor=lightblue]
	1861634193536 -> 1861634365696
	1861634365696 [label=AccumulateGrad]
	1861634366272 -> 1861634365648
	1861634193456 [label="conv_up3.0.bias
 (512)" fillcolor=lightblue]
	1861634193456 -> 1861634366272
	1861634366272 [label=AccumulateGrad]
	1861634365504 -> 1861634367232
	1861634365504 -> 1861633955360 [dir=none]
	1861633955360 [label="result
 (1, 128, 128, 128)" fillcolor=orange]
	1861634365504 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634365360 -> 1861634365504
	1861634365360 -> 1861634244672 [dir=none]
	1861634244672 [label="input
 (1, 128, 128, 128)" fillcolor=orange]
	1861634365360 -> 1861634180256 [dir=none]
	1861634180256 [label="weight
 (128, 128, 1, 1)" fillcolor=orange]
	1861634365360 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634371120 -> 1861634365360
	1861634364112 -> 1861634365360
	1861634180256 [label="layer2_1x1.0.weight
 (128, 128, 1, 1)" fillcolor=lightblue]
	1861634180256 -> 1861634364112
	1861634364112 [label=AccumulateGrad]
	1861634365024 -> 1861634365360
	1861634195136 [label="layer2_1x1.0.bias
 (128)" fillcolor=lightblue]
	1861634195136 -> 1861634365024
	1861634365024 [label=AccumulateGrad]
	1861634366128 -> 1861634367520
	1861634193056 [label="conv_up2.0.weight
 (256, 640, 3, 3)" fillcolor=lightblue]
	1861634193056 -> 1861634366128
	1861634366128 [label=AccumulateGrad]
	1861634368192 -> 1861634367520
	1861634192816 [label="conv_up2.0.bias
 (256)" fillcolor=lightblue]
	1861634192816 -> 1861634368192
	1861634368192 [label=AccumulateGrad]
	1861634368480 -> 1861634369152
	1861634368480 -> 1861633952320 [dir=none]
	1861633952320 [label="result
 (1, 64, 256, 256)" fillcolor=orange]
	1861634368480 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634366896 -> 1861634368480
	1861634366896 -> 1861634249232 [dir=none]
	1861634249232 [label="input
 (1, 64, 256, 256)" fillcolor=orange]
	1861634366896 -> 1861634180656 [dir=none]
	1861634180656 [label="weight
 (64, 64, 1, 1)" fillcolor=orange]
	1861634366896 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634367856 -> 1861634366896
	1861634365984 -> 1861634366896
	1861634180656 [label="layer1_1x1.0.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1861634180656 -> 1861634365984
	1861634365984 [label=AccumulateGrad]
	1861634366944 -> 1861634366896
	1861634189216 [label="layer1_1x1.0.bias
 (64)" fillcolor=lightblue]
	1861634189216 -> 1861634366944
	1861634366944 [label=AccumulateGrad]
	1861634369104 -> 1861634369488
	1861634192096 [label="conv_up1.0.weight
 (256, 320, 3, 3)" fillcolor=lightblue]
	1861634192096 -> 1861634369104
	1861634369104 [label=AccumulateGrad]
	1861634370112 -> 1861634369488
	1861634192016 [label="conv_up1.0.bias
 (256)" fillcolor=lightblue]
	1861634192016 -> 1861634370112
	1861634370112 [label=AccumulateGrad]
	1861634370400 -> 1861634370688
	1861634370400 -> 1861633955200 [dir=none]
	1861633955200 [label="result
 (1, 64, 512, 512)" fillcolor=orange]
	1861634370400 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634368816 -> 1861634370400
	1861634368816 -> 1861634257152 [dir=none]
	1861634257152 [label="input
 (1, 64, 512, 512)" fillcolor=orange]
	1861634368816 -> 1860786283328 [dir=none]
	1860786283328 [label="weight
 (64, 64, 1, 1)" fillcolor=orange]
	1861634368816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634363872 -> 1861634368816
	1861634367472 -> 1861634368816
	1860786283328 [label="layer0_1x1.0.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	1860786283328 -> 1861634367472
	1861634367472 [label=AccumulateGrad]
	1861634368864 -> 1861634368816
	1860786284128 [label="layer0_1x1.0.bias
 (64)" fillcolor=lightblue]
	1860786284128 -> 1861634368864
	1861634368864 [label=AccumulateGrad]
	1861634371024 -> 1861634370976
	1861634191696 [label="conv_up0.0.weight
 (128, 320, 3, 3)" fillcolor=lightblue]
	1861634191696 -> 1861634371024
	1861634371024 [label=AccumulateGrad]
	1861634371600 -> 1861634370976
	1861634191376 [label="conv_up0.0.bias
 (128)" fillcolor=lightblue]
	1861634191376 -> 1861634371600
	1861634371600 [label=AccumulateGrad]
	1861634371936 -> 1861634372608
	1861634371936 -> 1861633954720 [dir=none]
	1861633954720 [label="result
 (1, 64, 1024, 1024)" fillcolor=orange]
	1861634371936 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634370736 -> 1861634371936
	1861634370736 -> 1861634257552 [dir=none]
	1861634257552 [label="input
 (1, 64, 1024, 1024)" fillcolor=orange]
	1861634370736 -> 1861634190576 [dir=none]
	1861634190576 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	1861634370736 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634369440 -> 1861634370736
	1861634369440 -> 1861633952800 [dir=none]
	1861633952800 [label="result
 (1, 64, 1024, 1024)" fillcolor=orange]
	1861634369440 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	1861634364400 -> 1861634369440
	1861634364400 -> 1861634257472 [dir=none]
	1861634257472 [label="input
 (1, 3, 1024, 1024)" fillcolor=orange]
	1861634364400 -> 1861634191056 [dir=none]
	1861634191056 [label="weight
 (64, 3, 3, 3)" fillcolor=orange]
	1861634364400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1861634362000 -> 1861634364400
	1861634191056 [label="conv_original_size0.0.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	1861634191056 -> 1861634362000
	1861634362000 [label=AccumulateGrad]
	1861634361952 -> 1861634364400
	1861634190976 [label="conv_original_size0.0.bias
 (64)" fillcolor=lightblue]
	1861634190976 -> 1861634361952
	1861634361952 [label=AccumulateGrad]
	1861634370352 -> 1861634370736
	1861634190576 [label="conv_original_size1.0.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1861634190576 -> 1861634370352
	1861634370352 [label=AccumulateGrad]
	1861634371648 -> 1861634370736
	1861634190176 [label="conv_original_size1.0.bias
 (64)" fillcolor=lightblue]
	1861634190176 -> 1861634371648
	1861634371648 [label=AccumulateGrad]
	1861634372560 -> 1861634373472
	1861634189776 [label="conv_original_size2.0.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	1861634189776 -> 1861634372560
	1861634372560 [label=AccumulateGrad]
	1861634373520 -> 1861634373472
	1861634189696 [label="conv_original_size2.0.bias
 (64)" fillcolor=lightblue]
	1861634189696 -> 1861634373520
	1861634373520 [label=AccumulateGrad]
	1861634373184 -> 1861634373808
	1861634189296 [label="conv_last.weight
 (1, 64, 1, 1)" fillcolor=lightblue]
	1861634189296 -> 1861634373184
	1861634373184 [label=AccumulateGrad]
	1861634373232 -> 1861634373808
	1861634189056 [label="conv_last.bias
 (1)" fillcolor=lightblue]
	1861634189056 -> 1861634373232
	1861634373232 [label=AccumulateGrad]
	1861634373808 -> 1861634250272
}
